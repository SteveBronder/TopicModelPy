{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbssampler(matrix,nzw,nzm,nz,nm,max_iter,p_z,[ntopics,m,n])\n",
      "\n",
      "Wrapper for ``gibbssampler``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "matrix : input rank-2 array('i') with bounds (m,n)\n",
      "nzw : input rank-2 array('i') with bounds (n,ntopics)\n",
      "nzm : input rank-2 array('i') with bounds (ntopics,m)\n",
      "nz : input rank-1 array('i') with bounds (ntopics)\n",
      "nm : input rank-1 array('i') with bounds (m)\n",
      "max_iter : input int\n",
      "p_z : input rank-1 array('f') with bounds (ntopics)\n",
      "\n",
      "Other Parameters\n",
      "----------------\n",
      "ntopics : input int, optional\n",
      "    Default: shape(nzw,1)\n",
      "m : input int, optional\n",
      "    Default: shape(matrix,0)\n",
      "n : input int, optional\n",
      "    Default: shape(matrix,1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./fortran')\n",
    "import gibbsSampler\n",
    "print gibbsSampler.gibbs_sampler.gibbssampler.__doc__\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "random.seed(1234)\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def index_sample(p):\n",
    "    \"\"\"\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    \"\"\"\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    \"\"\"\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "\n",
    "class LdaSampler(object):\n",
    "\n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "\n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "\n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "\n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "\n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "\n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        \"\"\"\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "\n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = []\n",
    "        self.logL = []\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(self.matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                # Keep document, iterator for word, word index, and assignment\n",
    "                #self.topics.append([i,w,z])\n",
    "        \n",
    "        #self.topics = np.vstack(self.topics)\n",
    "    def _conditional_distribution(self, m, w):\n",
    "        \"\"\"\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "\n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        \"\"\"\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        \"\"\"\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "\n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "\n",
    "        return lik\n",
    "\n",
    "    def phi_theta(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding\n",
    "            [0] phi: Probability of topic by word\n",
    "            [1] theta: Probability of document by topic\n",
    "        \"\"\"\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "\n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "\n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        n_docs, vsize = self.matrix.shape\n",
    "\n",
    "\n",
    "\n",
    "        for iteration in xrange(maxiter + 2):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(self.matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "\n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "\n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "\n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "\n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "                \n",
    "    def runfort(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        M,N = self.matrix.shape\n",
    "\n",
    "        p_z = np.zeros(self.ntopics)\n",
    "        p_z += 1./self.ntopics\n",
    "        gibbsSampler.gibbs_sampler.gibbssampler(matrix = self.matrix, nzw = self.NZW.transpose(),\n",
    "                                                nzm = self.NZM.transpose(),\n",
    "                                                nz = self.NZ,\n",
    "                                                nm = self.NM,\n",
    "                                                max_iter = maxiter,\n",
    "                                                p_z = p_z)\n",
    "\n",
    "        #if iteration > burnin:\n",
    "        return self.phi_theta()\n",
    "                \n",
    "                \n",
    "\n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "\n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding\n",
    "                [0] Probability of topic by word\n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "            self.logL.append(self.loglikelihood())\n",
    "        return self.phi_theta(), self.logL\n",
    "\n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ\n",
    "        self.logL = self.logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "p=.5\n",
    "# sample files for train\n",
    "gen_sample = np.array(sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))))\n",
    "rand_sample = [ allfiles[i] for i in gen_sample ]\n",
    "#\n",
    "# take rest for test\n",
    "rand_sample2 = []\n",
    "for i in xrange(len(allfiles)):\n",
    "    if i not in gen_sample:\n",
    "        rand_sample2.append(allfiles[i])\n",
    "#\n",
    "# train data\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "#\n",
    "# test data\n",
    "np_array_list_test = []\n",
    "for file_ in rand_sample2:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    df['source'] = file_\n",
    "    np_array_list_test.append(df.as_matrix())\n",
    "    \n",
    "#\n",
    "# train data frame\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "train_frame = pd.DataFrame(comb_np_array)\n",
    "train_frame.columns = ['words','count','source']\n",
    "subless = (train_frame['words'].str.len() > 2)\n",
    "submore = (train_frame['words'].str.len() < 20)\n",
    "train_frame = train_frame.loc[subless]\n",
    "train_frame = train_frame.loc[submore]\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.loc[:, (train_frame.sum(axis = 0) > 5)]\n",
    "#\n",
    "\n",
    "# test data frame\n",
    "comb_np_array_test = np.vstack(np_array_list_test)\n",
    "test_frame = pd.DataFrame(comb_np_array_test)\n",
    "test_frame.columns = ['words','count','source']\n",
    "test_frame = test_frame.fillna(value=0)\n",
    "test_frame = test_frame.pivot(index = 'source', columns = 'words', values = 'count')\n",
    "test_frame = test_frame.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_frame.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0\n",
      "1 2\n",
      "2 2\n",
      "3 2\n",
      "4 2\n",
      "5 3\n",
      "6 3\n",
      "0 0\n",
      "1 0\n",
      "2 1\n",
      "3 1\n",
      "4 1\n",
      "5 1\n",
      "6 3\n",
      "7 3\n",
      "8 3\n",
      "9 3\n",
      "10 3\n"
     ]
    }
   ],
   "source": [
    "for m in xrange(testing_frame.shape[0]):\n",
    "    for i,w in enumerate(word_indices(testing_frame[m,:])):\n",
    "        print i,w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = LdaSampler(data = train_frame.values, ntopics = 4, alpha = .1, beta = .1)\n",
    "LDAtest = sampler.runfort(maxiter = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "testing_frame = np.array([[1,0,4,2],[2,4,0,5]])\n",
    "sampler = LdaSampler(data = testing_frame, ntopics = 4, alpha = .1, beta = .1)\n",
    "p_z = np.zeros(4) + .25\n",
    "M,N = testing_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gibbsSampler.gibbs_sampler.gibbssampler(matrix = sampler.matrix, nzw = sampler.NZW.transpose(),\n",
    "                                                nzm = sampler.NZM.transpose(),\n",
    "                                                nz = sampler.NZ,\n",
    "                                                nm = sampler.NM,\n",
    "                                                max_iter = 10,\n",
    "                                                p_z = p_z,\n",
    "                                                m = M,\n",
    "                                                n = N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.NZM"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
