{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "\n",
    "def index_sample(p):\n",
    "    '''\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    '''\n",
    "    return np.random.multinomial(1,p).argmax()\n",
    "\n",
    "def word_indices(vec):\n",
    "    '''\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    '''\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "    \n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "    \n",
    "class LdaSampler(object):\n",
    "    \n",
    "    def __init__(self, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "        \n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "            \n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "            \n",
    "        \n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    def _initialize(self,matrix):\n",
    "        '''\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "        \n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "        \n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "        \n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "        \n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        '''\n",
    "        ndocs, vsize = matrix.shape\n",
    "        \n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "    \n",
    "    def _conditional_distribution(self, m, w):\n",
    "        '''\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "        \n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        '''\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "    \n",
    "    def loglikelihood(self):\n",
    "        '''\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        '''\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "        \n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "        \n",
    "        return lik\n",
    "    \n",
    "    def phi(self):\n",
    "        '''\n",
    "        Desc: Compute the probability of a word given topic\n",
    "        Input: Self references\n",
    "        ???\n",
    "        ?Output: An array of size(1 X Words) containing the probabilities for a given topic\n",
    "        ???\n",
    "        '''\n",
    "        num = self.NZW + self.beta\n",
    "        num /= np.sum(num, axis = 1)[:, np.newaxis]\n",
    "        return num\n",
    "    \n",
    "    def run(self, matrix, maxiter = 30, burnin = 10):\n",
    "        '''\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "        \n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "               \n",
    "        Output: phi() the probability of a word given a document\n",
    "        '''\n",
    "        \n",
    "        n_docs, vsize = matrix.shape\n",
    "        self._initialize(matrix)\n",
    "        \n",
    "        for iteration in xrange(maxiter):\n",
    "            for burn in xrange(burnin):\n",
    "                for m in xrange(n_docs):\n",
    "                    for i,w in enumerate(word_indices(matrix[m,:])):\n",
    "                        z = self.topics[(m,i)]\n",
    "                    \n",
    "                        # Why minus one?\n",
    "                        self.NZM[m,z] -= 1\n",
    "                        self.NM[m] -= 1\n",
    "                        self.NZW[z,w] -= 1\n",
    "                        self.NZ[z] -= 1\n",
    "                    \n",
    "                        p_z = self._conditional_distribution(m,w)\n",
    "                        z = index_sample(p_z)\n",
    "                    \n",
    "                        self.NZM[m,z] += 1\n",
    "                        self.NM[m] += 1\n",
    "                        self.NZW[z,w] += 1\n",
    "                        self.NZ[z] += 1\n",
    "                        \n",
    "                yield self.phi()\n",
    "\n",
    "                \n",
    "                  \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "N_TOPICS = 10\n",
    "DOCUMENT_LENGTH = 100\n",
    "FOLDER = \"topicimg\"\n",
    "\n",
    "def vertical_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a vertical bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[:, topic_index] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def horizontal_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a horizontal bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[topic_index, :] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def save_document_image(filename, doc, zoom=2):\n",
    "    \"\"\"\n",
    "    Save document as an image.\n",
    "    doc must be a square matrix\n",
    "    \"\"\"\n",
    "    height, width = doc.shape\n",
    "    zoom = np.ones((width*zoom, width*zoom))\n",
    "    # imsave scales pixels between 0 and 255 automatically\n",
    "    sp.misc.imsave(filename, np.kron(doc, zoom))\n",
    "\n",
    "def gen_word_distribution(ntopics, document_length):\n",
    "    \"\"\"\n",
    "    Generate a word distribution for each of the ntopics.\n",
    "    \"\"\"\n",
    "    width = ntopics / 2\n",
    "    vsize = width ** 2\n",
    "    m = np.zeros((ntopics, vsize))\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k,:] = vertical_topic(width, k, document_length)\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k+width,:] = horizontal_topic(width, k, document_length)\n",
    "\n",
    "    m /= m.sum(axis=1)[:, np.newaxis] # turn counts into probabilities\n",
    "\n",
    "    return m\n",
    "\n",
    "def gen_document(word_dist, ntopics, vsize, length=DOCUMENT_LENGTH, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate a document:\n",
    "    1) Sample topic proportions from the Dirichlet distribution.\n",
    "    2) Sample a topic index from the Multinomial with the topic\n",
    "       proportions from 1).\n",
    "    3) Sample a word from the Multinomial corresponding to the topic\n",
    "       index from 2).\n",
    "    4) Go to 2) if need another word.\n",
    "    \"\"\"\n",
    "    theta = np.random.mtrand.dirichlet([alpha] * ntopics)\n",
    "    v = np.zeros(vsize)\n",
    "    for n in range(length):\n",
    "        z = index_sample(theta)\n",
    "        w = index_sample(word_dist[z,:])\n",
    "        v[w] += 1\n",
    "    return v\n",
    "\n",
    "def gen_documents(word_dist, ntopics, vsize, n=500):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix.\n",
    "    \"\"\"\n",
    "    m = np.zeros((n, vsize))\n",
    "    for i in xrange(n):\n",
    "        m[i, :] = gen_document(word_dist, ntopics, vsize)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if os.path.exists(FOLDER):\n",
    "    shutil.rmtree(FOLDER)\n",
    "os.mkdir(FOLDER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -10940613.7489\n",
      "Iteration 1\n",
      "Likelihood -10893431.8518\n",
      "Iteration 2\n",
      "Likelihood -10811639.7223\n",
      "Iteration 3\n",
      "Likelihood -10735797.9854\n",
      "Iteration 4\n",
      "Likelihood -10666011.6539\n",
      "Iteration 5\n",
      "Likelihood -10591724.4034\n",
      "Iteration 6\n",
      "Likelihood -10518229.2302\n",
      "Iteration 7\n",
      "Likelihood -10444880.6194\n",
      "Iteration 8\n",
      "Likelihood -10372297.044\n",
      "Iteration 9\n",
      "Likelihood -10301053.0435\n",
      "Iteration 10\n",
      "Likelihood -10230344.1454\n",
      "Iteration 11\n",
      "Likelihood -10158971.6791\n",
      "Iteration 12\n",
      "Likelihood -10086640.5294\n",
      "Iteration 13\n",
      "Likelihood -10012641.206\n",
      "Iteration 14\n",
      "Likelihood -9941714.0673\n",
      "Iteration 15\n",
      "Likelihood -9868821.70907\n",
      "Iteration 16\n",
      "Likelihood -9797512.35174\n",
      "Iteration 17\n",
      "Likelihood -9724779.86518\n",
      "Iteration 18\n",
      "Likelihood -9654301.46867\n",
      "Iteration 19\n",
      "Likelihood -9581852.81952\n",
      "Iteration 20\n",
      "Likelihood -9510058.9357\n",
      "Iteration 21\n",
      "Likelihood -9439599.46918\n",
      "Iteration 22\n",
      "Likelihood -9367880.27203\n",
      "Iteration 23\n",
      "Likelihood -9296062.68742\n",
      "Iteration 24\n",
      "Likelihood -9223015.61027\n",
      "Iteration 25\n",
      "Likelihood -9148658.93831\n",
      "Iteration 26\n",
      "Likelihood -9075695.5907\n",
      "Iteration 27\n",
      "Likelihood -9004143.4869\n",
      "Iteration 28\n",
      "Likelihood -8931516.86831\n",
      "Iteration 29\n",
      "Likelihood -8858307.60026\n",
      "Iteration 30\n",
      "Likelihood -8788875.37728\n",
      "Iteration 31\n",
      "Likelihood -8716819.26847\n",
      "Iteration 32\n",
      "Likelihood -8644756.32043\n",
      "Iteration 33\n",
      "Likelihood -8571638.28074\n",
      "Iteration 34\n",
      "Likelihood -8500845.1354\n",
      "Iteration 35\n",
      "Likelihood -8429779.01386\n",
      "Iteration 36\n",
      "Likelihood -8358544.95369\n",
      "Iteration 37\n",
      "Likelihood -8288321.64017\n",
      "Iteration 38\n",
      "Likelihood -8218034.5788\n",
      "Iteration 39\n",
      "Likelihood -8148707.59164\n",
      "Iteration 40\n",
      "Likelihood -8079378.53747\n",
      "Iteration 41\n",
      "Likelihood -8007187.56041\n",
      "Iteration 42\n",
      "Likelihood -7936307.90686\n",
      "Iteration 43\n",
      "Likelihood -7864695.48309\n",
      "Iteration 44\n",
      "Likelihood -7792949.85565\n",
      "Iteration 45\n",
      "Likelihood -7721763.89597\n",
      "Iteration 46\n",
      "Likelihood -7649651.20587\n",
      "Iteration 47\n",
      "Likelihood -7579246.82356\n",
      "Iteration 48\n",
      "Likelihood -7512159.55899\n",
      "Iteration 49\n",
      "Likelihood -7439467.01146\n",
      "Iteration 50\n",
      "Likelihood -7368876.19402\n",
      "Iteration 51\n",
      "Likelihood -7300000.75326\n",
      "Iteration 52\n",
      "Likelihood -7229233.73089\n",
      "Iteration 53\n",
      "Likelihood -7158070.5186\n",
      "Iteration 54\n",
      "Likelihood -7087242.20519\n",
      "Iteration 55\n",
      "Likelihood -7016213.2413\n",
      "Iteration 56\n",
      "Likelihood -6946208.40333\n",
      "Iteration 57\n",
      "Likelihood -6875523.54115\n",
      "Iteration 58\n",
      "Likelihood -6804843.31297\n",
      "Iteration 59\n",
      "Likelihood -6734000.50279\n",
      "Iteration 60\n",
      "Likelihood -6663953.91663\n",
      "Iteration 61\n",
      "Likelihood -6592588.49375\n",
      "Iteration 62\n",
      "Likelihood -6522072.43682\n",
      "Iteration 63\n",
      "Likelihood -6450911.12347\n",
      "Iteration 64\n",
      "Likelihood -6379805.43758\n",
      "Iteration 65\n",
      "Likelihood -6307112.20849\n",
      "Iteration 66\n",
      "Likelihood -6236634.14752\n",
      "Iteration 67\n",
      "Likelihood -6166244.44746\n",
      "Iteration 68\n",
      "Likelihood -6094247.01478\n",
      "Iteration 69\n",
      "Likelihood -6023039.58987\n",
      "Iteration 70\n",
      "Likelihood -5951916.9034\n",
      "Iteration 71\n",
      "Likelihood -5881544.04231\n",
      "Iteration 72\n",
      "Likelihood -5811256.99624\n",
      "Iteration 73\n",
      "Likelihood -5738117.35959\n",
      "Iteration 74\n",
      "Likelihood -5667337.81154\n",
      "Iteration 75\n",
      "Likelihood -5595475.04457\n",
      "Iteration 76\n",
      "Likelihood -5524300.30281\n",
      "Iteration 77\n",
      "Likelihood -5451799.29322\n",
      "Iteration 78\n",
      "Likelihood -5379920.99237\n",
      "Iteration 79\n",
      "Likelihood -5308498.79455\n",
      "Iteration 80\n",
      "Likelihood -5236959.60191\n",
      "Iteration 81\n",
      "Likelihood -5165525.99369\n",
      "Iteration 82\n",
      "Likelihood -5095267.96977\n",
      "Iteration 83\n",
      "Likelihood -5024257.11651\n",
      "Iteration 84\n",
      "Likelihood -4952814.63578\n"
     ]
    }
   ],
   "source": [
    "width = N_TOPICS / 2\n",
    "vsize = width ** 2\n",
    "word_dist = gen_word_distribution(N_TOPICS, DOCUMENT_LENGTH)\n",
    "matrix = gen_documents(word_dist, N_TOPICS, vsize)\n",
    "sampler = LdaSampler(N_TOPICS)\n",
    "\n",
    "for it, phi in enumerate(sampler.run(matrix)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood()\n",
    "\n",
    "    if it % 5 == 0:\n",
    "        for z in range(N_TOPICS):\n",
    "            save_document_image(\"topicimg/topic%d-%d.png\" % (it,z),\n",
    "            phi[z,:].reshape(width,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for it, phi in enumerate(sampler.run(matrix)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood()\n",
    "\n",
    "    if it % 5 == 0:\n",
    "        for z in range(N_TOPICS):\n",
    "            save_document_image(\"topicimg/topic%d-%d.png\" % (it,z),\n",
    "            phi[z,:].reshape(width,-1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
