{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "N_TOPICS = 10\n",
    "DOCUMENT_LENGTH = 100\n",
    "FOLDER = \"topicimg\"\n",
    "\n",
    "def vertical_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a vertical bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[:, topic_index] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def horizontal_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a horizontal bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[topic_index, :] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def save_document_image(filename, doc, zoom=2):\n",
    "    \"\"\"\n",
    "    Save document as an image.\n",
    "    doc must be a square matrix\n",
    "    \"\"\"\n",
    "    height, width = doc.shape\n",
    "    zoom = np.ones((width*zoom, width*zoom))\n",
    "    # imsave scales pixels between 0 and 255 automatically\n",
    "    sp.misc.imsave(filename, np.kron(doc, zoom))\n",
    "\n",
    "def gen_word_distribution(ntopics, document_length):\n",
    "    \"\"\"\n",
    "    Generate a word distribution for each of the ntopics.\n",
    "    \"\"\"\n",
    "    width = ntopics / 2\n",
    "    vsize = width ** 2\n",
    "    m = np.zeros((ntopics, vsize))\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k,:] = vertical_topic(width, k, document_length)\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k+width,:] = horizontal_topic(width, k, document_length)\n",
    "\n",
    "    m /= m.sum(axis=1)[:, np.newaxis] # turn counts into probabilities\n",
    "\n",
    "    return m\n",
    "\n",
    "def gen_document(word_dist, ntopics, vsize, length=DOCUMENT_LENGTH, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate a document:\n",
    "    1) Sample topic proportions from the Dirichlet distribution.\n",
    "    2) Sample a topic index from the Multinomial with the topic\n",
    "       proportions from 1).\n",
    "    3) Sample a word from the Multinomial corresponding to the topic\n",
    "       index from 2).\n",
    "    4) Go to 2) if need another word.\n",
    "    \"\"\"\n",
    "    theta = np.random.mtrand.dirichlet([alpha] * ntopics)\n",
    "    v = np.zeros(vsize)\n",
    "    for n in range(length):\n",
    "        z = index_sample(theta)\n",
    "        w = index_sample(word_dist[z,:])\n",
    "        v[w] += 1\n",
    "    return v\n",
    "\n",
    "def gen_documents(word_dist, ntopics, vsize, n=500):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix.\n",
    "    \"\"\"\n",
    "    m = np.zeros((n, vsize))\n",
    "    for i in xrange(n):\n",
    "        m[i, :] = gen_document(word_dist, ntopics, vsize)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if os.path.exists(FOLDER):\n",
    "    shutil.rmtree(FOLDER)\n",
    "os.mkdir(FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def index_sample(p):\n",
    "    '''\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    '''\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    '''\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    '''\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "    \n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "    \n",
    "class LdaSampler(object):\n",
    "    \n",
    "    def __init__(self, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "        \n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "            \n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "            \n",
    "        \n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    def _initialize(self,matrix):\n",
    "        '''\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "        \n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "        \n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "        \n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "        \n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        '''\n",
    "        ndocs, vsize = matrix.shape\n",
    "        \n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "    \n",
    "    def _conditional_distribution(self, m, w):\n",
    "        '''\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "        \n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        '''\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "    \n",
    "    def loglikelihood(self):\n",
    "        '''\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        '''\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "        \n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "        \n",
    "        return lik\n",
    "    \n",
    "    def phi_theta(self):\n",
    "        '''\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding \n",
    "            [0] phi: Probability of topic by word \n",
    "            [1] theta: Probability of document by topic\n",
    "        '''\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "        \n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "        \n",
    "    def run(self, matrix, maxiter = 30, burnin = 10):\n",
    "        '''\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "        \n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "               \n",
    "        Output: phi_theta() Two arrays, holding \n",
    "        [0] Probability of topic by word \n",
    "        [1] Probability of document by topic\n",
    "        '''\n",
    "        \n",
    "        n_docs, vsize = matrix.shape\n",
    "        self._initialize(matrix)\n",
    "    \n",
    "                    \n",
    "        for iteration in xrange(maxiter):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "                \n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "                  \n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "                    \n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "            \n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "    \n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "    \n",
    "    def update(self, matrix, maxiter = 30, burnin = 10):\n",
    "        for iteration, phi_theta in enumerate(self.run(matrix, maxiter , burnin)):\n",
    "            if iteration % 5 == 0:\n",
    "                self.prn(iteration)\n",
    "                self.prn(self.loglikelihood() )\n",
    "        \n",
    "\n",
    "                \n",
    "                  \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = LdaSampler(5)\n",
    "\n",
    "LDAtest = sampler.update(matrix,burnin = 30, maxiter = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.update(matrix,burnin = 30, maxiter = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'phi'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-468-6fb8e97d13dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mLDAtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'phi'"
     ]
    }
   ],
   "source": [
    "LDAtest.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list() takes at most 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-429-b25c2e873e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_TOPICS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_theta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburnin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-425-cf0906645e7f>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, matrix, maxiter, burnin)\u001b[0m\n\u001b[0;32m    211\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi_theta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-425-cf0906645e7f>\u001b[0m in \u001b[0;36mphi_theta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mnum_theta\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m# Not sure if this is kosher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_phi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list() takes at most 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "width = N_TOPICS / 2\n",
    "vsize = width ** 2\n",
    "word_dist = gen_word_distribution(N_TOPICS, DOCUMENT_LENGTH)\n",
    "matrix = gen_documents(word_dist, N_TOPICS, vsize)\n",
    "sampler = LdaSampler(N_TOPICS)\n",
    "\n",
    "for it, phi_theta in enumerate(sampler.run(matrix, maxiter = 30, burnin = 10)):\n",
    "    \n",
    "\n",
    "    if it % 5 == 0:\n",
    "        print \"Iteration\", it\n",
    "        print \"Likelihood\", sampler.loglikelihood()\n",
    "        for z in range(N_TOPICS):\n",
    "            save_document_image(\"topicimg/topic%d-%d.png\" % (it,z),\n",
    "            phi[z,:].reshape(width,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "\n",
    "p = .5\n",
    "rand_sample = [ allfiles[i] for i in sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))) ]\n",
    "rand_sample\n",
    "    \n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "    \n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "big_frame = pd.DataFrame(comb_np_array)\n",
    "big_frame.columns = ['words','count','source']\n",
    "\n",
    "big_frame = big_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "big_frame = big_frame.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>aaby</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aba</th>\n",
       "      <th>abab</th>\n",
       "      <th>ababab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>...</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zum</th>\n",
       "      <th>zupan</th>\n",
       "      <th>zurnal</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zx</th>\n",
       "      <th>zxp</th>\n",
       "      <th>zy</th>\n",
       "      <th>zygosity</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276708.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276722.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276742.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276818.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276825.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 9823 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words                                               aaby  aaron  aba  abab  \\\n",
       "source                                                                       \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...     0      0    0     0   \n",
       "\n",
       "words                                               ababab  abandon  \\\n",
       "source                                                                \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0        0   \n",
       "\n",
       "words                                               abandoned  abba  abbott  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0     0       0   \n",
       "\n",
       "words                                               abbreviated ...  zucker  \\\n",
       "source                                                          ...           \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0 ...       0   \n",
       "\n",
       "words                                               zum  zupan  zurnal  zvi  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0      0       0    0   \n",
       "\n",
       "words                                               zx  zxp  zy  zygosity  zz  \n",
       "source                                                                         \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...   0    0   0         0   0  \n",
       "\n",
       "[5 rows x 9823 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) > 2)]\n",
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) < 20)]\n",
    "big_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 34934)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -510547883685.0\n",
      "Iteration 1\n",
      "Likelihood -510550294175.0\n",
      "Iteration 2\n",
      "Likelihood -510573907380.0\n",
      "Iteration 3\n",
      "Likelihood -510628679589.0\n",
      "Iteration 4\n",
      "Likelihood -510633809299.0\n",
      "Iteration 5\n",
      "Likelihood -510659645673.0\n",
      "Iteration 6\n",
      "Likelihood -510665424219.0\n",
      "Iteration 7\n",
      "Likelihood -510651208299.0\n",
      "Iteration 8\n",
      "Likelihood -510644705702.0\n",
      "Iteration 9\n",
      "Likelihood -510650551356.0\n",
      "Iteration 10\n",
      "Likelihood -510653431940.0\n",
      "Iteration 11\n",
      "Likelihood -510650468845.0\n",
      "Iteration 12\n",
      "Likelihood -510638814662.0\n",
      "Iteration 13\n",
      "Likelihood -510624150253.0\n",
      "Iteration 14\n",
      "Likelihood -510619044655.0\n",
      "Iteration 15\n",
      "Likelihood -510614262016.0\n",
      "Iteration 16\n",
      "Likelihood -510611064296.0\n",
      "Iteration 17\n",
      "Likelihood -510606525451.0\n",
      "Iteration 18\n",
      "Likelihood -510605447122.0\n",
      "Iteration 19\n",
      "Likelihood -510604683425.0\n",
      "Iteration 20\n",
      "Likelihood -510606042584.0\n",
      "Iteration 21\n",
      "Likelihood -510605111489.0\n",
      "Iteration 22\n",
      "Likelihood -510604103147.0\n",
      "Iteration 23\n",
      "Likelihood -510602930467.0\n",
      "Iteration 24\n",
      "Likelihood -510600059783.0\n",
      "Iteration 25\n",
      "Likelihood -510603047426.0\n",
      "Iteration 26\n",
      "Likelihood -510597222399.0\n",
      "Iteration 27\n",
      "Likelihood -510592808046.0\n",
      "Iteration 28\n",
      "Likelihood -510589654256.0\n",
      "Iteration 29\n",
      "Likelihood -510588293961.0\n",
      "Iteration 30\n",
      "Likelihood -510584653991.0\n",
      "Iteration 31\n",
      "Likelihood -510588068010.0\n",
      "Iteration 32\n",
      "Likelihood -510584984194.0\n",
      "Iteration 33\n",
      "Likelihood -510584763083.0\n",
      "Iteration 34\n",
      "Likelihood -510582635038.0\n",
      "Iteration 35\n",
      "Likelihood -510582938759.0\n",
      "Iteration 36\n",
      "Likelihood -510582210963.0\n",
      "Iteration 37\n",
      "Likelihood -510582954618.0\n",
      "Iteration 38\n",
      "Likelihood -510587939783.0\n",
      "Iteration 39\n",
      "Likelihood -510587821278.0\n"
     ]
    }
   ],
   "source": [
    "for it, phi in enumerate(sampler.run(big_frame.values, maxiter = 30, burnin = 10)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_conditional_distribution() takes exactly 3 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-08230b270384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conditional_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: _conditional_distribution() takes exactly 3 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "sampler._conditional_distribution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
