{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "N_TOPICS = 10\n",
    "DOCUMENT_LENGTH = 100\n",
    "FOLDER = \"topicimg\"\n",
    "\n",
    "def vertical_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a vertical bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[:, topic_index] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def horizontal_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a horizontal bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[topic_index, :] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def save_document_image(filename, doc, zoom=2):\n",
    "    \"\"\"\n",
    "    Save document as an image.\n",
    "    doc must be a square matrix\n",
    "    \"\"\"\n",
    "    height, width = doc.shape\n",
    "    zoom = np.ones((width*zoom, width*zoom))\n",
    "    # imsave scales pixels between 0 and 255 automatically\n",
    "    sp.misc.imsave(filename, np.kron(doc, zoom))\n",
    "\n",
    "def gen_word_distribution(ntopics, document_length):\n",
    "    \"\"\"\n",
    "    Generate a word distribution for each of the ntopics.\n",
    "    \"\"\"\n",
    "    width = ntopics / 2\n",
    "    vsize = width ** 2\n",
    "    m = np.zeros((ntopics, vsize))\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k,:] = vertical_topic(width, k, document_length)\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k+width,:] = horizontal_topic(width, k, document_length)\n",
    "\n",
    "    m /= m.sum(axis=1)[:, np.newaxis] # turn counts into probabilities\n",
    "\n",
    "    return m\n",
    "\n",
    "def gen_document(word_dist, ntopics, vsize, length=DOCUMENT_LENGTH, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate a document:\n",
    "    1) Sample topic proportions from the Dirichlet distribution.\n",
    "    2) Sample a topic index from the Multinomial with the topic\n",
    "       proportions from 1).\n",
    "    3) Sample a word from the Multinomial corresponding to the topic\n",
    "       index from 2).\n",
    "    4) Go to 2) if need another word.\n",
    "    \"\"\"\n",
    "    theta = np.random.mtrand.dirichlet([alpha] * ntopics)\n",
    "    v = np.zeros(vsize)\n",
    "    for n in range(length):\n",
    "        z = index_sample(theta)\n",
    "        w = index_sample(word_dist[z,:])\n",
    "        v[w] += 1\n",
    "    return v\n",
    "\n",
    "def gen_documents(word_dist, ntopics, vsize, n=500):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix.\n",
    "    \"\"\"\n",
    "    m = np.zeros((n, vsize))\n",
    "    for i in xrange(n):\n",
    "        m[i, :] = gen_document(word_dist, ntopics, vsize)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if os.path.exists(FOLDER):\n",
    "    shutil.rmtree(FOLDER)\n",
    "os.mkdir(FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  14.,  16.,   7.,\n",
       "         0.,   0.,   9.,   0.,  16.,   2.,   0.,   0.,  13.,   0.,   4.,\n",
       "         0.,   0.,  10.,   0.,   0.,   1.,   2.,   0.,   0.,   0.,   0.,\n",
       "        15.,   1.,   0.,   0.,   6.,   0.,   2.,   0.,   7.,   0.,   2.,\n",
       "         0.,  11.,  14.,   0.,  18.,   5.,   1.,  15.,   0.,   1.,   0.,\n",
       "         0.,   0.,   3.,   0.,   1.,  18.,   0.,   3.,  22.,   0.,  12.,\n",
       "         7.,   1.,  14.,   1.,   2.,   0.,   7.,   2.,  13.,   4.,   0.,\n",
       "         0.,   4.,   0.,   4.,   5.,   0.,   2.,  13.,   2.,   1.,   0.,\n",
       "        19.,   0.,   1.,   0.,   1.,   1.,   0.,   0.,   0.,  22.,   7.,\n",
       "         0.,  16.,   0.,   1.,   3.,   0.,  17.,   1.,   0.,   2.,  10.,\n",
       "         0.,  12.,   2.,   0.,   1.,   0.,   0.,  12.,   5.,   0.,   3.,\n",
       "         0.,   0.,   0.,  10.,   0.,   7.,   1.,   4.,  16.,   0.,   0.,\n",
       "         0.,   0.,  14.,  10.,  10.,   0.,   0.,   7.,   0.,   0.,   0.,\n",
       "         0.,   2.,   0.,  12.,   0.,   2.,   1.,   0.,   0.,   0.,   1.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   2.,   2.,   0.,   0.,   0.,\n",
       "         1.,   2.,   0.,   8.,   0.,   0.,   6.,   4.,   2.,   0.,   0.,\n",
       "         0.,   0.,   0.,   5.,   5.,   1.,   2.,   0.,   0.,   5.,   0.,\n",
       "        11.,   2.,   0.,   1.,   1.,   8.,   5.,  19.,   0.,   6.,   0.,\n",
       "         0.,   0.,   0.,   3.,   0.,   4.,   0.,   5.,   0.,  19.,  20.,\n",
       "        15.,   8.,   0.,   0.,   9.,   0.,  20.,   1.,   8.,  10.,   5.,\n",
       "         0.,   0.,   1.,   0.,   0.,   4.,   8.,   3.,   1.,   9.,   6.,\n",
       "         0.,  14.,   0.,   3.,   8.,   1.,   1.,   5.,   0.,  12.,   1.,\n",
       "        12.,   5.,   0.,   0.,   7.,   1.,  16.,   0.,   0.,   0.,   3.,\n",
       "         0.,   0.,   2.,   0.,   3.,   9.,   0.,  12.,  19.,   1.,   0.,\n",
       "         0.,   0.,   0.,   6.,   0.,   0.,  10.,   0.,   0.,   0.,  11.,\n",
       "         0.,   0.,   0.,   0.,   0.,  17.,   2.,   0.,   0.,   0.,   0.,\n",
       "        14.,   0.,   0.,   0.,   0.,   0.,   1.,   9.,   2.,  14.,   0.,\n",
       "         0.,   0.,   4.,   0.,   0.,  11.,  11.,  15.,   0.,  19.,   0.,\n",
       "         5.,   1.,   0.,  21.,  18.,   0.,  12.,  12.,  22.,   0.,   0.,\n",
       "         5.,   1.,   0.,   2.,   0.,   0.,   6.,  10.,   4.,   4.,   1.,\n",
       "         1.,  11.,   0.,  11.,   0.,   0.,   2.,   0.,  20.,  23.,   3.,\n",
       "         1.,   8.,  14.,  13.,  15.,   4.,   5.,   0.,  15.,   0.,   7.,\n",
       "         0.,   0.,   0.,   7.,   0.,   2.,   2.,  23.,   0.,   6.,   9.,\n",
       "         6.,   3.,   0.,   0.,   0.,   5.,   2.,  11.,   0.,   0.,   0.,\n",
       "         0.,   2.,  11.,  18.,   0.,   8.,  15.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   2.,   1.,   0.,   0.,   0.,   0.,  10.,   3.,  19.,\n",
       "        19.,   3.,   2.,   8.,   6.,   0.,   0.,  17.,   8.,   7.,   6.,\n",
       "         8.,   9.,  10.,   3.,   0.,   2.,   5.,   5.,   0.,  14.,   0.,\n",
       "         7.,   3.,   2.,   0.,   8.,  14.,   4.,   0.,   0.,   0.,   8.,\n",
       "         0.,  18.,   0.,   0.,   2.,  20.,   0.,   0.,   0.,  10.,   1.,\n",
       "         2.,   3.,  10.,   0.,   0.,   0.,   2.,   3.,   1.,   0.,   0.,\n",
       "        14.,   3.,   1.,   2.,   0.,   0.,   5.,   0.,   0.,   0.,   0.,\n",
       "         0.,   6.,   7.,   1.,   0.,   6.,   0.,  18.,   3.,   0.,   0.,\n",
       "         7.,   0.,   2.,   0.,   0.,   0.,   1.,   0.,   7.,   1.,   0.,\n",
       "         2.,   0.,   3.,   0.,   0.,  14.,   0.,   1.,   3.,   0.,   2.,\n",
       "        14.,   0.,  11.,   0.,   0.])"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,1]\n",
    "#test2 = word_indices(matrix[:,1])\n",
    "matrix[:,1].nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def index_sample(p):\n",
    "    '''\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    '''\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    '''\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    '''\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "    \n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "    \n",
    "class LdaSampler(object):\n",
    "    \n",
    "    def __init__(self, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "        \n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "            \n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "            \n",
    "        \n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "    \n",
    "    def _initialize(self,matrix):\n",
    "        '''\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "        \n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "        \n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "        \n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "        \n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        '''\n",
    "        ndocs, vsize = matrix.shape\n",
    "        \n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "    \n",
    "    def _conditional_distribution(self, m, w):\n",
    "        '''\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "        \n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        '''\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "    \n",
    "    def loglikelihood(self):\n",
    "        '''\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        '''\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "        \n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "        \n",
    "        return lik\n",
    "    \n",
    "    def phi_theta(self):\n",
    "        '''\n",
    "        Desc: Compute the probability of a word given topic\n",
    "        Input: Self references\n",
    "        Output: An array of size(Topic X Docs) containing the probabilities for a given topic\n",
    "        '''\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "        \n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "        \n",
    "    def run(self, matrix, maxiter = 30, burnin = 10):\n",
    "        '''\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "        \n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "               \n",
    "        Output: phi_theta() Two arrays, holding \n",
    "        [0] Probability of topic by word \n",
    "        [1] Probability of document by topic\n",
    "        '''\n",
    "        \n",
    "        n_docs, vsize = matrix.shape\n",
    "        self._initialize(matrix)\n",
    "    \n",
    "                    \n",
    "        for iteration in xrange(maxiter):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "                \n",
    "                    # Why minus one?\n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "                  \n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "                    \n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "            \n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "                \n",
    "                  \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 10)"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1 = sampler.run(matrix, maxiter = 30, burnin = 10)\n",
    "test2 = test1.next()\n",
    "test2[1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test2 = word_indices(matrix[:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  3.,   0.,   2.,   2.,   3.,   0.,   3.,  10.,   0.,   0.,   5.,\n",
       "         0.,  12.,   0.,   0.,   0.,  19.,   0.,   2.,   0.,   0.,   5.,\n",
       "         2.,   5.,   9.,   0.,   0.,   0.,   0.,   0.,   2.,   1.,   0.,\n",
       "         0.,  19.,  15.,   0.,   1.,   0.,   3.,   0.,  12.,   0.,   0.,\n",
       "         0.,   4.,  27.,   0.,  20.,   0.,  11.,   0.,   2.,   2.,   0.,\n",
       "         0.,   5.,   0.,   0.,   0.,  19.,   0.,   2.,   0.,   1.,  10.,\n",
       "         0.,   3.,   0.,   7.,   0.,   3.,   0.,   1.,   9.,   0.,   9.,\n",
       "        22.,   4.,   1.,   6.,  14.,   1.,   2.,  21.,   0.,   0.,   0.,\n",
       "         0.,   0.,   3.,   0.,   1.,  16.,   0.,   0.,   0.,  25.,   0.,\n",
       "         0.,   6.,   2.,   3.,   1.,   0.,   3.,   0.,   0.,   9.,   8.,\n",
       "         0.,  10.,   2.,   0.,   2.,   1.,  11.,  16.,   0.,   2.,   6.,\n",
       "         4.,   0.,   0.,   4.,   0.,   7.,   3.,   5.,  11.,   0.,   0.,\n",
       "         0.,   0.,   0.,  16.,  10.,   1.,   2.,   6.,   0.,   0.,   0.,\n",
       "         0.,   2.,   0.,   0.,   0.,   8.,   5.,   0.,   0.,   2.,   4.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   1.,   4.,   3.,   0.,   0.,\n",
       "         0.,   3.,   0.,   0.,   0.,   0.,   4.,   8.,   0.,   0.,   0.,\n",
       "         0.,   0.,  18.,   5.,  10.,   0.,  17.,   0.,  13.,   1.,  12.,\n",
       "        13.,   1.,  22.,   8.,   3.,  11.,   1.,  26.,   0.,   3.,   0.,\n",
       "         0.,  10.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,  15.,  13.,\n",
       "         2.,   0.,   0.,  11.,   0.,   0.,  22.,   4.,   9.,   0.,   0.,\n",
       "         1.,   8.,   1.,   0.,   0.,   1.,   0.,  11.,   0.,   3.,  11.,\n",
       "         0.,  21.,   1.,   2.,   6.,   1.,  14.,   0.,   1.,  18.,   2.,\n",
       "         8.,   6.,   0.,   0.,  11.,   0.,  11.,   0.,   2.,   0.,  26.,\n",
       "         0.,   0.,   0.,   1.,   0.,   0.,  13.,  13.,   8.,   0.,   9.,\n",
       "         0.,   0.,   0.,   0.,   1.,   7.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   1.,   1.,   0.,   4.,   0.,   1.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  17.,   0.,   3.,   0.,   0.,\n",
       "        20.,   5.,   8.,   0.,   0.,   4.,   0.,   0.,   0.,  22.,  24.,\n",
       "         2.,   0.,   0.,   7.,   2.,  12.,  19.,   2.,   0.,   4.,   0.,\n",
       "         0.,   3.,   0.,   5.,   0.,   3.,   4.,   3.,   2.,   0.,   2.,\n",
       "        13.,   0.,   8.,   8.,   0.,   0.,   4.,   2.,  22.,   0.,   0.,\n",
       "         0.,   6.,   0.,  10.,  10.,   4.,   0.,   3.,   7.,   1.,   2.,\n",
       "         0.,   0.,   0.,   5.,   1.,   4.,   4.,  16.,   1.,   7.,  16.,\n",
       "         7.,   1.,   5.,   0.,  22.,   5.,   1.,  21.,   1.,   0.,   0.,\n",
       "         5.,   1.,  14.,  18.,   0.,  12.,  15.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   3.,   0.,   1.,   0.,\n",
       "         0.,   4.,   2.,   0.,  10.,   0.,   1.,   0.,   1.,   1.,   0.,\n",
       "         7.,  13.,  15.,  14.,   1.,   0.,   9.,   5.,   8.,   0.,   0.,\n",
       "         7.,   7.,   1.,   0.,   6.,   2.,   6.,   1.,   0.,   0.,   1.,\n",
       "         3.,  13.,   0.,   1.,   0.,   4.,   0.,   4.,   0.,   3.,  10.,\n",
       "         2.,   3.,   0.,   5.,   0.,   0.,   0.,   1.,   1.,   0.,   8.,\n",
       "        24.,   0.,   1.,   0.,   0.,   0.,   1.,   0.,  11.,   0.,   0.,\n",
       "        10.,  13.,   4.,   0.,   0.,   0.,   0.,   0.,  13.,   0.,   0.,\n",
       "         5.,   0.,   5.,   0.,   0.,   9.,   1.,   0.,   2.,   0.,   1.,\n",
       "         0.,   1.,   4.,  16.,   4.,   1.,  22.,  15.,   3.,   0.,   0.,\n",
       "         5.,   0.,   7.,  20.,  15.])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler = LdaSampler(N_TOPICS)\n",
    "test = sampler.run(matrix, maxiter = 30, burnin = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1 = test.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07707858,  0.94640769, -0.17142008,  0.22437816, -0.18331186,\n",
       "        -0.15457338,  0.31277376, -0.16983451,  0.08782083, -0.17815876,\n",
       "        -0.14624913,  0.19742345, -0.12167278,  1.1947478 , -0.13217719,\n",
       "        -0.10641165, -0.10720444, -0.12206917,  0.70183332, -0.18529383,\n",
       "        -0.17776236, -0.09570905, -0.18945595, -0.14129422, -0.20570806],\n",
       "       [-0.13882273, -0.14388987, -0.13189764,  0.29019508, -0.16601638,\n",
       "        -0.16179377, -0.14929482, -0.16719872,  0.65925175, -0.14608563,\n",
       "        -0.15334853, -0.15824677, -0.14557892, -0.09997466, -0.1411874 ,\n",
       "        -0.10909552, -0.11618951, -0.14270754,  0.01015117, -0.15115277,\n",
       "        -0.07531458,  0.94149143, -0.09659657,  1.2635926 ,  0.42971033],\n",
       "       [-0.11008657, -0.01612684, -0.14412995,  0.30738255,  1.04836105,\n",
       "        -0.17272639, -0.15560743, -0.179146  , -0.14451902, -0.11728431,\n",
       "        -0.18692734, -0.12195312, -0.14354635, -0.13440327,  0.61494018,\n",
       "         0.0130532 ,  0.6515125 , -0.12448205, -0.10755763,  1.00906527,\n",
       "        -0.179146  , -0.15191129, -0.1589145 , -0.19645949, -0.09938722],\n",
       "       [-0.1078461 ,  0.67583401,  0.9310938 ,  0.57865861,  0.6688597 ,\n",
       "        -0.0987795 , -0.15713123, -0.16061839, -0.14597234,  0.02048123,\n",
       "        -0.17968151, -0.17503197, -0.14713472, -0.14085784, -0.11854004,\n",
       "        -0.11830757,  0.51310008,  0.3164245 , -0.13272114, -0.09320005,\n",
       "        -0.1736371 , -0.16643032, -0.17479949, -0.210136  , -0.20362664],\n",
       "       [-0.12310592, -0.17286091,  0.65990702, -0.12461364, -0.17210705,\n",
       "        -0.17286091, -0.19195879,  0.56818696, -0.1296394 , -0.16456841,\n",
       "        -0.1525066 , -0.17034803,  1.04990577, -0.01781631, -0.14346023,\n",
       "        -0.1256188 , -0.16105038, -0.13717804,  0.78730996, -0.21482598,\n",
       "        -0.18391758, -0.15979394,  1.09614273, -0.21105667, -0.23216484],\n",
       "       [ 0.11180142, -0.17329075,  0.10512411, -0.19303237, -0.20174191,\n",
       "        -0.17125853,  0.37163594, -0.18722601, -0.16312963, -0.19129046,\n",
       "         1.47513427,  1.02368994, -0.08125998, -0.11000145, -0.11958194,\n",
       "        -0.10796923, -0.1367107 ,  0.98449702, -0.1805487 , -0.16864567,\n",
       "        -0.19941936,  0.00699666, -0.18577442, -0.27635361, -0.23164465],\n",
       "       [-0.13817096, -0.14769554, -0.13687949, -0.17126483, -0.21243038,\n",
       "         0.31029139,  0.74293325,  0.63444992,  0.11850835,  0.0999435 ,\n",
       "        -0.11540883,  0.44912422,  0.17969166, -0.11992897,  0.06588102,\n",
       "        -0.05583986,  0.12302849,  0.31594156, -0.12509484, -0.13687949,\n",
       "        -0.13042215, -0.11314876, -0.10523852, -0.15835015, -0.1730406 ],\n",
       "       [-0.1453481 , -0.14156534, -0.17488967, -0.16480231,  0.09764928,\n",
       "        -0.13291903, -0.1395839 , -0.16750428, -0.135621  ,  1.11719355,\n",
       "        -0.15309376, -0.15795731, -0.15327389, -0.12427272,  0.51987751,\n",
       "        -0.11868864, -0.15291363, -0.1224714 , -0.135621  ,  0.55374223,\n",
       "        -0.11688733,  0.20338647, -0.08896695, -0.14786995,  1.18240115],\n",
       "       [ 0.99010873, -0.18856923,  0.04312518, -0.15393053, -0.16971038,\n",
       "        -0.12256326, -0.17817762, -0.1506591 , -0.14546329, -0.16393727,\n",
       "        -0.13815068, -0.14815741,  0.23633215,  0.1052824 , -0.14642548,\n",
       "         0.05428654, -0.1212162 , -0.13988261, -0.14296161, -0.18183393,\n",
       "         1.19755605, -0.12544982,  0.6239007 ,  0.32908689, -0.1625902 ],\n",
       "       [ 0.1800019 , -0.14339587, -0.15882297, -0.15844205, -0.20434244,\n",
       "         1.07572612, -0.13577754,  0.29865727,  0.16381297, -0.04911913,\n",
       "         0.58243977, -0.15177602, -0.12206457, -0.13939625, -0.12968289,\n",
       "         0.95916579, -0.12149319, -0.12530235, -0.13958671, -0.14930007,\n",
       "         0.31656033, -0.15044281, -0.13977716, -0.18415389, -0.17348824]])"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list() takes at most 1 argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-429-b25c2e873e04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msampler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN_TOPICS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphi_theta\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburnin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-425-cf0906645e7f>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, matrix, maxiter, burnin)\u001b[0m\n\u001b[0;32m    211\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNZ\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mphi_theta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-425-cf0906645e7f>\u001b[0m in \u001b[0;36mphi_theta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mnum_theta\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_theta\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m# Not sure if this is kosher\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_phi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnum_theta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list() takes at most 1 argument (2 given)"
     ]
    }
   ],
   "source": [
    "width = N_TOPICS / 2\n",
    "vsize = width ** 2\n",
    "word_dist = gen_word_distribution(N_TOPICS, DOCUMENT_LENGTH)\n",
    "matrix = gen_documents(word_dist, N_TOPICS, vsize)\n",
    "sampler = LdaSampler(N_TOPICS)\n",
    "\n",
    "for it, phi_theta in enumerate(sampler.run(matrix, maxiter = 30, burnin = 10)):\n",
    "    \n",
    "\n",
    "    if it % 5 == 0:\n",
    "        print \"Iteration\", it\n",
    "        print \"Likelihood\", sampler.loglikelihood()\n",
    "        for z in range(N_TOPICS):\n",
    "            save_document_image(\"topicimg/topic%d-%d.png\" % (it,z),\n",
    "            phi[z,:].reshape(width,-1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "\n",
    "p = .5\n",
    "rand_sample = [ allfiles[i] for i in sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))) ]\n",
    "rand_sample\n",
    "    \n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "    \n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "big_frame = pd.DataFrame(comb_np_array)\n",
    "big_frame.columns = ['words','count','source']\n",
    "\n",
    "big_frame = big_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "big_frame = big_frame.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>aaby</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aba</th>\n",
       "      <th>abab</th>\n",
       "      <th>ababab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abba</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>...</th>\n",
       "      <th>zucker</th>\n",
       "      <th>zum</th>\n",
       "      <th>zupan</th>\n",
       "      <th>zurnal</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zx</th>\n",
       "      <th>zxp</th>\n",
       "      <th>zy</th>\n",
       "      <th>zygosity</th>\n",
       "      <th>zz</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276708.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276722.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276742.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276818.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276825.CSV</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 9823 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words                                               aaby  aaron  aba  abab  \\\n",
       "source                                                                       \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...     0      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...     0      0    0     0   \n",
       "\n",
       "words                                               ababab  abandon  \\\n",
       "source                                                                \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0        0   \n",
       "\n",
       "words                                               abandoned  abba  abbott  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0     0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0     0       0   \n",
       "\n",
       "words                                               abbreviated ...  zucker  \\\n",
       "source                                                          ...           \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0 ...       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0 ...       0   \n",
       "\n",
       "words                                               zum  zupan  zurnal  zvi  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0      0       0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0      0       0    0   \n",
       "\n",
       "words                                               zx  zxp  zy  zygosity  zz  \n",
       "source                                                                         \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...   0    0   0         0   0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...   0    0   0         0   0  \n",
       "\n",
       "[5 rows x 9823 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) > 2)]\n",
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) < 20)]\n",
    "big_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 34934)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Likelihood -510547883685.0\n",
      "Iteration 1\n",
      "Likelihood -510550294175.0\n",
      "Iteration 2\n",
      "Likelihood -510573907380.0\n",
      "Iteration 3\n",
      "Likelihood -510628679589.0\n",
      "Iteration 4\n",
      "Likelihood -510633809299.0\n",
      "Iteration 5\n",
      "Likelihood -510659645673.0\n",
      "Iteration 6\n",
      "Likelihood -510665424219.0\n",
      "Iteration 7\n",
      "Likelihood -510651208299.0\n",
      "Iteration 8\n",
      "Likelihood -510644705702.0\n",
      "Iteration 9\n",
      "Likelihood -510650551356.0\n",
      "Iteration 10\n",
      "Likelihood -510653431940.0\n",
      "Iteration 11\n",
      "Likelihood -510650468845.0\n",
      "Iteration 12\n",
      "Likelihood -510638814662.0\n",
      "Iteration 13\n",
      "Likelihood -510624150253.0\n",
      "Iteration 14\n",
      "Likelihood -510619044655.0\n",
      "Iteration 15\n",
      "Likelihood -510614262016.0\n",
      "Iteration 16\n",
      "Likelihood -510611064296.0\n",
      "Iteration 17\n",
      "Likelihood -510606525451.0\n",
      "Iteration 18\n",
      "Likelihood -510605447122.0\n",
      "Iteration 19\n",
      "Likelihood -510604683425.0\n",
      "Iteration 20\n",
      "Likelihood -510606042584.0\n",
      "Iteration 21\n",
      "Likelihood -510605111489.0\n",
      "Iteration 22\n",
      "Likelihood -510604103147.0\n",
      "Iteration 23\n",
      "Likelihood -510602930467.0\n",
      "Iteration 24\n",
      "Likelihood -510600059783.0\n",
      "Iteration 25\n",
      "Likelihood -510603047426.0\n",
      "Iteration 26\n",
      "Likelihood -510597222399.0\n",
      "Iteration 27\n",
      "Likelihood -510592808046.0\n",
      "Iteration 28\n",
      "Likelihood -510589654256.0\n",
      "Iteration 29\n",
      "Likelihood -510588293961.0\n",
      "Iteration 30\n",
      "Likelihood -510584653991.0\n",
      "Iteration 31\n",
      "Likelihood -510588068010.0\n",
      "Iteration 32\n",
      "Likelihood -510584984194.0\n",
      "Iteration 33\n",
      "Likelihood -510584763083.0\n",
      "Iteration 34\n",
      "Likelihood -510582635038.0\n",
      "Iteration 35\n",
      "Likelihood -510582938759.0\n",
      "Iteration 36\n",
      "Likelihood -510582210963.0\n",
      "Iteration 37\n",
      "Likelihood -510582954618.0\n",
      "Iteration 38\n",
      "Likelihood -510587939783.0\n",
      "Iteration 39\n",
      "Likelihood -510587821278.0\n"
     ]
    }
   ],
   "source": [
    "for it, phi in enumerate(sampler.run(big_frame.values, maxiter = 30, burnin = 10)):\n",
    "    print \"Iteration\", it\n",
    "    print \"Likelihood\", sampler.loglikelihood()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_conditional_distribution() takes exactly 3 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-108-08230b270384>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msampler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conditional_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: _conditional_distribution() takes exactly 3 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "sampler._conditional_distribution()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
