{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "N_TOPICS = 10\n",
    "DOCUMENT_LENGTH = 100\n",
    "FOLDER = \"topicimg\"\n",
    "\n",
    "def vertical_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a vertical bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[:, topic_index] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def horizontal_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a horizontal bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[topic_index, :] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def save_document_image(filename, doc, zoom=2):\n",
    "    \"\"\"\n",
    "    Save document as an image.\n",
    "    doc must be a square matrix\n",
    "    \"\"\"\n",
    "    height, width = doc.shape\n",
    "    zoom = np.ones((width*zoom, width*zoom))\n",
    "    # imsave scales pixels between 0 and 255 automatically\n",
    "    sp.misc.imsave(filename, np.kron(doc, zoom))\n",
    "\n",
    "def gen_word_distribution(ntopics, document_length):\n",
    "    \"\"\"\n",
    "    Generate a word distribution for each of the ntopics.\n",
    "    \"\"\"\n",
    "    width = ntopics / 2\n",
    "    vsize = width ** 2\n",
    "    m = np.zeros((ntopics, vsize))\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k,:] = vertical_topic(width, k, document_length)\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k+width,:] = horizontal_topic(width, k, document_length)\n",
    "\n",
    "    m /= m.sum(axis=1)[:, np.newaxis] # turn counts into probabilities\n",
    "\n",
    "    return m\n",
    "\n",
    "def gen_document(word_dist, ntopics, vsize, length=DOCUMENT_LENGTH, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate a document:\n",
    "    1) Sample topic proportions from the Dirichlet distribution.\n",
    "    2) Sample a topic index from the Multinomial with the topic\n",
    "       proportions from 1).\n",
    "    3) Sample a word from the Multinomial corresponding to the topic\n",
    "       index from 2).\n",
    "    4) Go to 2) if need another word.\n",
    "    \"\"\"\n",
    "    theta = np.random.mtrand.dirichlet([alpha] * ntopics)\n",
    "    v = np.zeros(vsize)\n",
    "    for n in range(length):\n",
    "        z = index_sample(theta)\n",
    "        w = index_sample(word_dist[z,:])\n",
    "        v[w] += 1\n",
    "    return v\n",
    "\n",
    "def gen_documents(word_dist, ntopics, vsize, n=500):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix.\n",
    "    \"\"\"\n",
    "    m = np.zeros((n, vsize))\n",
    "    for i in xrange(n):\n",
    "        m[i, :] = gen_document(word_dist, ntopics, vsize)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if os.path.exists(FOLDER):\n",
    "    shutil.rmtree(FOLDER)\n",
    "os.mkdir(FOLDER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def index_sample(p):\n",
    "    '''\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    '''\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    '''\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    '''\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "    \n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "    \n",
    "class LdaSampler(object):\n",
    "    \n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "        \n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "            \n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "        \n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        '''\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "        \n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "        \n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "        \n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "        \n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        '''\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "        \n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(self.matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "    \n",
    "    def _conditional_distribution(self, m, w):\n",
    "        '''\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "        \n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        '''\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "    \n",
    "    def loglikelihood(self):\n",
    "        '''\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        '''\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "        \n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "        \n",
    "        return lik\n",
    "    \n",
    "    def phi_theta(self):\n",
    "        '''\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding \n",
    "            [0] phi: Probability of topic by word \n",
    "            [1] theta: Probability of document by topic\n",
    "        '''\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "        \n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "        \n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        '''\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "        \n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "               \n",
    "        Output: phi_theta() Two arrays, holding \n",
    "        [0] Probability of topic by word \n",
    "        [1] Probability of document by topic\n",
    "        '''\n",
    "        \n",
    "        n_docs, vsize = self.matrix.shape\n",
    "        \n",
    "    \n",
    "                    \n",
    "        for iteration in xrange(maxiter):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(self.matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "                \n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "                  \n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "                    \n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "            \n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "    \n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "    \n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding \n",
    "                [0] Probability of topic by word \n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "        return self.phi_theta()\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ\n",
    "        \n",
    "\n",
    "                \n",
    "                  \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = LdaSampler(data = matrix, ntopics = 5, alpha = .1, beta = .1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-10794391.6783\n",
      "1\n",
      "-10745505.6829\n",
      "2\n",
      "-10687161.3311\n",
      "3\n",
      "-10627083.6979\n",
      "4\n",
      "-10568265.7428\n",
      "5\n",
      "-10509290.0355\n",
      "6\n",
      "-10451636.6077\n",
      "7\n",
      "-10396654.4931\n",
      "8\n",
      "-10341041.371\n",
      "9\n",
      "-10288298.7021\n",
      "10\n",
      "-10239659.5004\n",
      "11\n",
      "-10191174.421\n",
      "12\n",
      "-10142940.428\n",
      "13\n",
      "-10096202.3979\n",
      "14\n",
      "-10048151.8639\n",
      "15\n",
      "-9999155.39838\n",
      "16\n",
      "-9951478.65329\n",
      "17\n",
      "-9903321.1695\n",
      "18\n",
      "-9855751.61107\n"
     ]
    }
   ],
   "source": [
    "LDAtest = sampler.update(maxiter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-9763120.72008\n",
      "1\n",
      "-9718356.74564\n",
      "2\n",
      "-9673326.71194\n",
      "3\n",
      "-9629062.19547\n"
     ]
    }
   ],
   "source": [
    "LDAtest = sampler.update(maxiter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width = N_TOPICS / 2\n",
    "vsize = width ** 2\n",
    "word_dist = gen_word_distribution(N_TOPICS, DOCUMENT_LENGTH)\n",
    "matrix = gen_documents(word_dist, N_TOPICS, vsize)\n",
    "sampler = LdaSampler(N_TOPICS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "\n",
    "p = .5\n",
    "rand_sample = [ allfiles[i] for i in sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))) ]\n",
    "rand_sample\n",
    "    \n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "    \n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "big_frame = pd.DataFrame(comb_np_array)\n",
    "big_frame.columns = ['words','count','source']\n",
    "big_frame = big_frame.fillna(value = 0)\n",
    "big_frame = big_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "big_frame = big_frame.fillna(value = 0)\n",
    "\n",
    "#big_test = big_frame.head(n=10000)\n",
    "#big_test = big_test.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "#big_test = big_test.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>aac</th>\n",
       "      <th>aacm</th>\n",
       "      <th>aalen</th>\n",
       "      <th>aam</th>\n",
       "      <th>aamse</th>\n",
       "      <th>aar</th>\n",
       "      <th>aare</th>\n",
       "      <th>aaron</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandonment</th>\n",
       "      <th>abb</th>\n",
       "      <th>abbey</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abbreviations</th>\n",
       "      <th>aberdeen</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ables</th>\n",
       "      <th>ably</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276708.CSV</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276722.CSV</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276742.CSV</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276818.CSV</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276953.CSV</th>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td> 0</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 10069 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words                                                  aac  aacm  aalen  aam  \\\n",
       "source                                                                         \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV    0     0      0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV    0     0      0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV    0     0      0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV    0     0      0    0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV    0     0      0    0   \n",
       "\n",
       "words                                                  aamse  aar  aare  \\\n",
       "source                                                                    \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV      0    0     0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV      0    0     0   \n",
       "\n",
       "words                                                  aaron  abandon  \\\n",
       "source                                                                  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV      0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV      0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV      0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV      0        0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV      0        0   \n",
       "\n",
       "words                                                  abandoned  abandonment  \\\n",
       "source                                                                          \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV          0            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV          0            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV          0            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV          0            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV          0            0   \n",
       "\n",
       "words                                                  abb  abbey  abbott  \\\n",
       "source                                                                      \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV    0      0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV    0      0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV    0      0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV    0      0       0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV    0      0       0   \n",
       "\n",
       "words                                                  abbreviated  \\\n",
       "source                                                               \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV            0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV            0   \n",
       "\n",
       "words                                                  abbreviations  \\\n",
       "source                                                                 \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV              0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV              0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV              0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV              0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV              0   \n",
       "\n",
       "words                                                  aberdeen  abilities  \\\n",
       "source                                                                       \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV         0          0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV         0          0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV         0          0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV         0          0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV         0          0   \n",
       "\n",
       "words                                                  ables  ably      \n",
       "source                                                                  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276708.CSV      0     0 ...  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276722.CSV      0     0 ...  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276742.CSV      0     0 ...  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276818.CSV      0     0 ...  \n",
       "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV      0     0 ...  \n",
       "\n",
       "[5 rows x 10069 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) > 2)]\n",
    "big_frame = big_frame.loc[:, (big_frame.sum(axis = 0) < 20)]\n",
    "big_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 34934)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = LdaSampler(data = big_frame.values, ntopics = 5, alpha = .1, beta = .1)\n",
    "LDAtest = sampler.update(maxiter = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 5)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAtest[1].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
