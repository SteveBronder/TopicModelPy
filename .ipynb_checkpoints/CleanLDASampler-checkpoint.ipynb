{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def index_sample(p):\n",
    "    \"\"\"\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    \"\"\"\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    \"\"\"\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "\n",
    "class LdaSampler(object):\n",
    "\n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "\n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "\n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "\n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "\n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "\n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        \"\"\"\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "\n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "        self.logL = []\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(self.matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "\n",
    "    def _conditional_distribution(self, m, w):\n",
    "        \"\"\"\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "\n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        \"\"\"\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        \"\"\"\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "\n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "\n",
    "        return lik\n",
    "\n",
    "    def phi_theta(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding\n",
    "            [0] phi: Probability of topic by word\n",
    "            [1] theta: Probability of document by topic\n",
    "        \"\"\"\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "\n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "\n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        n_docs, vsize = self.matrix.shape\n",
    "\n",
    "\n",
    "\n",
    "        for iteration in xrange(maxiter + 2):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(self.matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "\n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "\n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "\n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "\n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "\n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding\n",
    "                [0] Probability of topic by word\n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "            self.logL.append(self.loglikelihood())\n",
    "        return self.phi_theta(), self.logL\n",
    "\n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ\n",
    "        self.logL = self.logL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "\n",
    "# sample files for train\n",
    "gen_sample = np.array(sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))))\n",
    "rand_sample = [ allfiles[i] for i in gen_sample ]\n",
    "\n",
    "# take rest for test\n",
    "rand_sample2 = []\n",
    "for i in xrange(len(allfiles)):\n",
    "    if i not in gen_sample:\n",
    "        rand_sample2.append(allfiles[i])\n",
    "\n",
    "# train data\n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "\n",
    "# test data\n",
    "np_array_list_test = []\n",
    "for file_ in rand_sample2:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    df['source'] = file_\n",
    "    np_array_list_test.append(df.as_matrix())\n",
    "    \n",
    "# train data frame\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "train_frame = pd.DataFrame(comb_np_array)\n",
    "train_frame.columns = ['words','count','source']\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.loc[:, (train_frame.sum(axis = 0) > 5)]\n",
    "train_frame = train_frame.loc[:, (train_frame.sum(axis = 0) < 20)]\n",
    "# test data frame\n",
    "comb_np_array_test = np.vstack(np_array_list_test)\n",
    "test_frame = pd.DataFrame(comb_np_array_test)\n",
    "test_frame.columns = ['words','count','source']\n",
    "test_frame = test_frame.fillna(value=0)\n",
    "test_frame = test_frame.pivot(index = 'source', columns = 'words', values = 'count')\n",
    "test_frame = test_frame.fillna(value = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>0</th>\n",
       "      <th>aalen</th>\n",
       "      <th>aamse</th>\n",
       "      <th>abab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abbott</th>\n",
       "      <th>abbreviated</th>\n",
       "      <th>abbreviations</th>\n",
       "      <th>aberrant</th>\n",
       "      <th>ables</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zou</th>\n",
       "      <th>zq</th>\n",
       "      <th>zr</th>\n",
       "      <th>zu</th>\n",
       "      <th>zucchini</th>\n",
       "      <th>zupan</th>\n",
       "      <th>zv</th>\n",
       "      <th>zvi</th>\n",
       "      <th>zygosity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>source</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276742.CSV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276818.CSV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276825.CSV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276843.CSV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_folder/wordcounts/wordcounts_10.2307_2276892.CSV</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 4863 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words                                                 0  aalen  aamse  abab  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...  0.0    0.0    0.0   0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0    0.0    0.0   0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0    0.0    0.0   0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0    0.0    0.0   0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0    0.0    0.0   0.0   \n",
       "\n",
       "words                                               abandon  abbott  \\\n",
       "source                                                                \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...      0.0     0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...      0.0     0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...      0.0     0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...      0.0     0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...      0.0     0.0   \n",
       "\n",
       "words                                               abbreviated  \\\n",
       "source                                                            \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...          0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...          0.0   \n",
       "\n",
       "words                                               abbreviations  aberrant  \\\n",
       "source                                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...            0.0       0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0.0       0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0.0       0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0.0       0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...            0.0       0.0   \n",
       "\n",
       "words                                               ables    ...     zone  \\\n",
       "source                                                       ...            \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...    0.0    ...      0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0.0    ...      0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0.0    ...      0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0.0    ...      0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...    0.0    ...      0.0   \n",
       "\n",
       "words                                               zou   zq   zr   zu  \\\n",
       "source                                                                   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...  0.0  0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0  0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0  0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0  0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...  0.0  0.0  0.0  0.0   \n",
       "\n",
       "words                                               zucchini  zupan   zv  zvi  \\\n",
       "source                                                                          \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0.0    0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0    0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0    0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0    0.0  0.0  0.0   \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0    0.0  0.0  0.0   \n",
       "\n",
       "words                                               zygosity  \n",
       "source                                                        \n",
       "data_folder/wordcounts/wordcounts_10.2307_22767...       0.0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0  \n",
       "data_folder/wordcounts/wordcounts_10.2307_22768...       0.0  \n",
       "\n",
       "[5 rows x 4863 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "sampler = LdaSampler(data = train_frame.values, ntopics = 5, alpha = .1, beta = .1)\n",
    "LDAtest = sampler.update(maxiter = 350,burnin = 200)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"time elapsed: \", elapsed, \" seconds\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[ -8.40243416e-04,  -1.34948185e-03,   8.57218030e-04, ...,\n",
       "           -1.17973571e-03,  -5.85624199e-04,  -2.79232408e-03],\n",
       "         [  9.12966868e-06,  -1.90810076e-03,  -8.21670182e-05, ...,\n",
       "            7.31286462e-03,   9.12966868e-06,   9.32139173e-03],\n",
       "         [ -9.52811757e-04,  -2.10773510e-03,  -8.56568145e-04, ...,\n",
       "           -2.01149149e-03,  -1.24154259e-03,   9.62436118e-06],\n",
       "         [  3.48203923e-03,  -4.45423251e-03,   1.69637808e-03, ...,\n",
       "            9.92033967e-06,   4.67247999e-03,  -4.35502912e-03],\n",
       "         [ -9.27537440e-04,   1.18912384e-02,  -1.03175513e-03, ...,\n",
       "           -3.84563276e-03,  -2.07393203e-03,  -1.65706127e-03]]),\n",
       "  array([[ -4.53066667,  10.58933333,  -1.83733333,  -2.05066667,\n",
       "           -1.17066667],\n",
       "         [ -0.94146341,  -0.90894309,   4.13170732,  -0.74634146,\n",
       "           -0.53495935],\n",
       "         [ -1.70901961,   6.23607843,  -1.08156863,  -1.14431373,\n",
       "           -1.30117647],\n",
       "         ..., \n",
       "         [  3.82567394,   0.757638  ,  -1.30141207,  -0.94197689,\n",
       "           -1.33992298],\n",
       "         [ -1.52578241,   6.82295082,  -1.51684054,  -1.49597615,\n",
       "           -1.28435171],\n",
       "         [  7.46972477,  -0.49357798,  -1.86972477,  -2.53027523,\n",
       "           -1.57614679]])),\n",
       " [-2122984118.4360802,\n",
       "  -2122910861.7736721,\n",
       "  -2122830068.4249992,\n",
       "  -2122802673.2024217,\n",
       "  -2122841054.3591802,\n",
       "  -2122799776.1462305,\n",
       "  -2122779747.01368,\n",
       "  -2122778240.9968166,\n",
       "  -2122745032.8460753,\n",
       "  -2122798635.5986104,\n",
       "  -2122707980.3140314,\n",
       "  -2122769505.5395794,\n",
       "  -2122803494.4406323,\n",
       "  -2122904680.9825647,\n",
       "  -2122899059.5928626,\n",
       "  -2122851847.9650908,\n",
       "  -2122789999.8270819,\n",
       "  -2122678559.3454242,\n",
       "  -2122661948.8198121,\n",
       "  -2122612372.1146584,\n",
       "  -2122556342.8452823])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDAtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "omega = np.cross(X, np.linalg.qr(theta.transpose * theta) *theta.transpose)\n",
    "\n",
    "row_sums = omega.sum(axis=1)\n",
    "omega = omega / row_sums[:, numpy.newaxis]\n",
    "\n",
    "\n",
    "#for idx in vec.nonzero()[0]:\n",
    "#        for i in xrange(int(vec[idx])):\n",
    "#            yield idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
