{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbssampler(matrix,nzw,nzm,nz,nm,p_z,topics,alpha,beta,lik,[ntopics,max_iter,m,n,top_size])\n",
      "\n",
      "Wrapper for ``gibbssampler``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "matrix : input rank-2 array('q') with bounds (n,m)\n",
      "nzw : input rank-2 array('q') with bounds (ntopics,n)\n",
      "nzm : input rank-2 array('q') with bounds (ntopics,m)\n",
      "nz : input rank-1 array('q') with bounds (ntopics)\n",
      "nm : input rank-1 array('q') with bounds (m)\n",
      "p_z : input rank-1 array('d') with bounds (ntopics)\n",
      "topics : input rank-1 array('q') with bounds (top_size)\n",
      "alpha : input float\n",
      "beta : input float\n",
      "lik : in/output rank-1 array('d') with bounds (max_iter)\n",
      "\n",
      "Other Parameters\n",
      "----------------\n",
      "ntopics : input long, optional\n",
      "    Default: shape(nzw,0)\n",
      "max_iter : input long, optional\n",
      "    Default: len(lik)\n",
      "m : input long, optional\n",
      "    Default: shape(matrix,1)\n",
      "n : input long, optional\n",
      "    Default: shape(matrix,0)\n",
      "top_size : input long, optional\n",
      "    Default: len(topics)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./fortran')\n",
    "import gibbsSampler6th\n",
    "print gibbsSampler6th.gibbs_sampler.gibbssampler.__doc__\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "random.seed(1234)\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def index_sample(p):\n",
    "    \"\"\"\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    \"\"\"\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    \"\"\"\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "\n",
    "class LdaSampler(object):\n",
    "\n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        #if not isinstance(alpha, float):\n",
    "        #    raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "\n",
    "        #if not isinstance(beta, float):\n",
    "        #    raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "\n",
    "        #if not isinstance(ntopics, int):\n",
    "        #    raise Exception(\" The number of topics must be an integer\")\n",
    "\n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "\n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "\n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "\n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "\n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        \"\"\"\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "\n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = []\n",
    "        self.logL = []\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for n in xrange(vsize):\n",
    "                if self.matrix[m,n] == 0:\n",
    "                    continue\n",
    "                for w in xrange(self.matrix[m,n]):\n",
    "                # Initialize a random topic for each word\n",
    "                    z = np.random.randint(self.ntopics)\n",
    "                    self.topics.append(z)\n",
    "                    self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,n] += 1\n",
    "                    self.NZ[z] += 1\n",
    "                # Keep document, iterator for word, word index, and assignment\n",
    "                #self.topics.append([i,w,z])\n",
    "        \n",
    "        self.topics = np.vstack(self.topics)\n",
    "    def _conditional_distribution(self, m, n):\n",
    "        \"\"\"\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "\n",
    "        Output: p_z: An array size(1 X ntopics) containing\n",
    "                  probabilities for topics of word\n",
    "        \n",
    "        The formula is:\n",
    "        ((n_{k,-i}^(t) + \\beta_t)/\\sum_{t=1}^V(n_{k,-i}^(t) + \\beta_t)) *\n",
    "        ((n_{m,-i}^(t) + \\alpha_k)/(\\sum_{k=1}^K(n_m^k + \\alpha_k) - 1))\n",
    "        \"\"\"\n",
    "        vsize = self.matrix[m,:].nonzero()[0].size\n",
    "        p_z = np.zeros(self.ntopics)\n",
    "        for ii in xrange(self.ntopics):\n",
    "            p_z[ii] = (self.NZM[m,ii] + self.alpha) \\\n",
    "            *(self.NZW[ii,n] + self.beta) \\\n",
    "            / (self.NZ[ii] + vsize * self.beta)\n",
    "        \n",
    "        p_z /= np.sum(p_z)\n",
    "\n",
    "        return p_z\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        \"\"\"\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.matrix[m,:].nonzero()[0].size\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "\n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "\n",
    "        return lik\n",
    "\n",
    "    def phi_theta(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding\n",
    "            [0] phi: Probability of topic by word\n",
    "            [1] theta: Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 0)[np.newaxis,:]\n",
    "\n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:,np.newaxis ]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "\n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        n_docs, vsize = self.matrix.shape\n",
    "        topics2 = self.topics\n",
    "\n",
    "\n",
    "        for iteration in xrange(maxiter + 2):\n",
    "            # Idea: After each iteration we now want to\n",
    "            # make assignments relative to the newly generated topics\n",
    "            if iteration > 1:\n",
    "                self.topics = topics2\n",
    "            for m in xrange(n_docs):\n",
    "                for n in xrange(vsize):\n",
    "                    if self.matrix[m,n] == 0:\n",
    "                        continue\n",
    "                    for w in xrange(self.matrix[m,n]):\n",
    "                        \n",
    "                        z = self.topics[m,n]\n",
    "                    \n",
    "                        self.NZM[m,z] -= 1\n",
    "                        self.NM[m] -= 1\n",
    "                        self.NZW[z,n] -= 1\n",
    "                        self.NZ[z] -= 1\n",
    "\n",
    "                        p_z = self._conditional_distribution(m,n)\n",
    "                        # Choosing a random topic row\n",
    "                        ind_z = np.random.randint(self.ntopics)\n",
    "                    \n",
    "                        # Sampling random topic\n",
    "                        z = index_sample(p_z)\n",
    "                        \n",
    "                        #Self.topics needs to change after we iterate over this word\n",
    "                        # Otherwise at each iteration we subtract one from that space w*n times\n",
    "                        # giving us a negative number\n",
    "                        topics2[m,n] = z\n",
    "\n",
    "                        self.NZM[m,z] += 1\n",
    "                        self.NM[m] += 1\n",
    "                        self.NZW[z,n] += 1\n",
    "                        self.NZ[z] += 1\n",
    "\n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "                \n",
    "    def runfort(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        M,N = self.matrix.shape\n",
    "\n",
    "        p_z = np.zeros(self.ntopics)\n",
    "        p_z += 1./self.ntopics\n",
    "        \n",
    " \n",
    "        # Make everything fortan contiguous\n",
    "        p_z=p_z.flatten() # Flatten array (Make 1-D)\n",
    "        p_z=p_z.reshape(self.ntopics, order='F')\n",
    "        run_matrix = np.array(self.matrix.transpose(),order='F')\n",
    "        run_NZM = np.array(self.NZM.transpose(),order='F')\n",
    "        run_NZW = np.array(self.NZW,order='F')\n",
    "        run_NZ = np.array(self.NZ,order='F')\n",
    "        run_NM = np.array(self.NM,order='F')\n",
    "        # index starts at 1 in fortran\n",
    "        run_topics = np.array(self.topics,order='F') + 1\n",
    "        run_topics2 = run_topics\n",
    "        \n",
    "        \n",
    "        topics2 = self.topics.transpose()\n",
    "        loglik = np.zeros(maxiter)\n",
    "        \n",
    "        gibbsSampler6th.gibbs_sampler.gibbssampler(matrix = run_matrix,\n",
    "                                                nzw = run_NZW,\n",
    "                                                nzm = run_NZM,\n",
    "                                                nz = run_NZ,\n",
    "                                                nm = run_NM,\n",
    "                                                max_iter = maxiter,\n",
    "                                                p_z = p_z,\n",
    "                                                m = M,\n",
    "                                                n = N,\n",
    "                                                topics = run_topics,\n",
    "                                                alpha = self.alpha,\n",
    "                                                beta = self.beta,\n",
    "                                                lik = loglik)\n",
    "                                                \n",
    "        \n",
    "        self.NZM = run_NZM.transpose()\n",
    "        self.matrix = run_matrix.transpose()\n",
    "        self.NZW = run_NZW\n",
    "        self.NZ = run_NZ\n",
    "        self.NM = run_NM\n",
    "\n",
    "        #if iteration > burnin:\n",
    "        return self.phi_theta(),loglik\n",
    "                \n",
    "                \n",
    "\n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "\n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding\n",
    "                [0] Probability of topic by word\n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "            self.logL.append(self.loglikelihood())\n",
    "        return self.phi_theta(), self.logL\n",
    "\n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ\n",
    "        self.logL = self.logL\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Little data for testing purposes\n",
    "testing_frame = np.array([[1,0,4,2],[2,4,0,5]])\n",
    "sampler = LdaSampler(data = testing_frame, ntopics = 4, alpha =.1,\n",
    "                     beta = .1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LDA_fort_test = sampler.runfort(maxiter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "p=.2\n",
    "# sample files for train\n",
    "gen_sample = np.array(sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))))\n",
    "rand_sample = [ allfiles[i] for i in gen_sample ]\n",
    "#\n",
    "# take rest for test\n",
    "rand_sample2 = []\n",
    "for i in xrange(len(allfiles)):\n",
    "    if i not in gen_sample:\n",
    "        rand_sample2.append(allfiles[i])\n",
    "#\n",
    "# train data\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "#\n",
    "# test data\n",
    "np_array_list_test = []\n",
    "for file_ in rand_sample2:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    df['source'] = file_\n",
    "    np_array_list_test.append(df.as_matrix())\n",
    "    \n",
    "#\n",
    "# train data frame\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "train_frame = pd.DataFrame(comb_np_array)\n",
    "train_frame.columns = ['words','count','source']\n",
    "subless = (train_frame['words'].str.len() > 2)\n",
    "submore = (train_frame['words'].str.len() < 20)\n",
    "train_frame = train_frame.loc[subless]\n",
    "train_frame = train_frame.loc[submore]\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.loc[:, (train_frame.sum(axis = 0) > 10)]\n",
    "#\n",
    "\n",
    "# test data frame\n",
    "comb_np_array_test = np.vstack(np_array_list_test)\n",
    "test_frame = pd.DataFrame(comb_np_array_test)\n",
    "test_frame.columns = ['words','count','source']\n",
    "test_frame = test_frame.fillna(value=0)\n",
    "test_frame = test_frame.pivot(index = 'source', columns = 'words', values = 'count')\n",
    "test_frame = test_frame.fillna(value = 0)\n",
    "\n",
    "train_frame1 = train_frame.values.astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'abb', u'ability', u'able', u'about', u'above', u'absence',\n",
       "       u'absolute', u'abstract', u'academic', u'academy',\n",
       "       ...\n",
       "       u'yobs', u'yohai', u'york', u'you', u'young', u'your', u'youth',\n",
       "       u'zeger', u'zero', u'zin'],\n",
       "      dtype='object', name=u'words', length=3569)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler1 = LdaSampler(data = train_frame1, ntopics = 8, alpha = .0001, beta = .0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_lda = sampler1.runfort( maxiter =10000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_theta = test_lda[0]\n",
    "likelihood = test_lda[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(likelihood)\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11097853,  0.13842481,  0.11336517, ...,  0.13603818,\n",
       "         0.13245823,  0.12410501],\n",
       "       [ 0.09428575,  0.1157143 ,  0.1342857 , ...,  0.12999999,\n",
       "         0.12000001,  0.13571427],\n",
       "       [ 0.1065574 ,  0.11475411,  0.11967214, ...,  0.14590161,\n",
       "         0.12622951,  0.14590161],\n",
       "       ..., \n",
       "       [ 0.11598441,  0.13060428,  0.11890839, ...,  0.11988304,\n",
       "         0.12573099,  0.11500975],\n",
       "       [ 0.14728678,  0.11111114,  0.12403101, ...,  0.08785537,\n",
       "         0.13695088,  0.13695088],\n",
       "       [ 0.13166855,  0.12712826,  0.11464246, ...,  0.12939841,\n",
       "         0.12826333,  0.12939841]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi_theta[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theta = pd.DataFrame(data=phi_theta[0], columns = train_frame.columns)\n",
    "phi = pd.DataFrame(data=phi_theta[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.110979</td>\n",
       "      <td>0.138425</td>\n",
       "      <td>0.113365</td>\n",
       "      <td>0.118138</td>\n",
       "      <td>0.126492</td>\n",
       "      <td>0.136038</td>\n",
       "      <td>0.132458</td>\n",
       "      <td>0.124105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.094286</td>\n",
       "      <td>0.115714</td>\n",
       "      <td>0.134286</td>\n",
       "      <td>0.148571</td>\n",
       "      <td>0.121429</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.135714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106557</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.119672</td>\n",
       "      <td>0.126230</td>\n",
       "      <td>0.114754</td>\n",
       "      <td>0.145902</td>\n",
       "      <td>0.126230</td>\n",
       "      <td>0.145902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.117560</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.119792</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.119792</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.121280</td>\n",
       "      <td>0.124256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.113449</td>\n",
       "      <td>0.116194</td>\n",
       "      <td>0.110704</td>\n",
       "      <td>0.137237</td>\n",
       "      <td>0.139982</td>\n",
       "      <td>0.141812</td>\n",
       "      <td>0.127173</td>\n",
       "      <td>0.113449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.125850</td>\n",
       "      <td>0.108844</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.113946</td>\n",
       "      <td>0.146258</td>\n",
       "      <td>0.113946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.118613</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.144161</td>\n",
       "      <td>0.098540</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>0.140511</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.133212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.123214</td>\n",
       "      <td>0.118651</td>\n",
       "      <td>0.130754</td>\n",
       "      <td>0.128373</td>\n",
       "      <td>0.119246</td>\n",
       "      <td>0.132341</td>\n",
       "      <td>0.120238</td>\n",
       "      <td>0.127183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.118497</td>\n",
       "      <td>0.104046</td>\n",
       "      <td>0.121387</td>\n",
       "      <td>0.127168</td>\n",
       "      <td>0.144509</td>\n",
       "      <td>0.101156</td>\n",
       "      <td>0.135838</td>\n",
       "      <td>0.147399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.122476</td>\n",
       "      <td>0.109018</td>\n",
       "      <td>0.142665</td>\n",
       "      <td>0.113055</td>\n",
       "      <td>0.126514</td>\n",
       "      <td>0.118439</td>\n",
       "      <td>0.148048</td>\n",
       "      <td>0.119785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.222221</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.069445</td>\n",
       "      <td>0.097223</td>\n",
       "      <td>0.152777</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.069445</td>\n",
       "      <td>0.138889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147343</td>\n",
       "      <td>0.101449</td>\n",
       "      <td>0.144927</td>\n",
       "      <td>0.115942</td>\n",
       "      <td>0.118358</td>\n",
       "      <td>0.140097</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.120773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.105166</td>\n",
       "      <td>0.092251</td>\n",
       "      <td>0.140221</td>\n",
       "      <td>0.125461</td>\n",
       "      <td>0.132841</td>\n",
       "      <td>0.145756</td>\n",
       "      <td>0.136531</td>\n",
       "      <td>0.121771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.122378</td>\n",
       "      <td>0.143357</td>\n",
       "      <td>0.125874</td>\n",
       "      <td>0.108392</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.119118</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.142647</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.132353</td>\n",
       "      <td>0.105882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.132143</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.096429</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.130357</td>\n",
       "      <td>0.126786</td>\n",
       "      <td>0.121429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.116149</td>\n",
       "      <td>0.128238</td>\n",
       "      <td>0.118307</td>\n",
       "      <td>0.123489</td>\n",
       "      <td>0.133851</td>\n",
       "      <td>0.132988</td>\n",
       "      <td>0.129965</td>\n",
       "      <td>0.117012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.137313</td>\n",
       "      <td>0.125373</td>\n",
       "      <td>0.095522</td>\n",
       "      <td>0.143284</td>\n",
       "      <td>0.167164</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.095522</td>\n",
       "      <td>0.146269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.124386</td>\n",
       "      <td>0.130933</td>\n",
       "      <td>0.116203</td>\n",
       "      <td>0.139116</td>\n",
       "      <td>0.130933</td>\n",
       "      <td>0.130933</td>\n",
       "      <td>0.096563</td>\n",
       "      <td>0.130933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.098765</td>\n",
       "      <td>0.138272</td>\n",
       "      <td>0.153086</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.096296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.130579</td>\n",
       "      <td>0.109091</td>\n",
       "      <td>0.117355</td>\n",
       "      <td>0.143802</td>\n",
       "      <td>0.135537</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.132231</td>\n",
       "      <td>0.090909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.119018</td>\n",
       "      <td>0.123313</td>\n",
       "      <td>0.119939</td>\n",
       "      <td>0.135583</td>\n",
       "      <td>0.126687</td>\n",
       "      <td>0.118098</td>\n",
       "      <td>0.129755</td>\n",
       "      <td>0.127607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.175373</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>0.100746</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.186567</td>\n",
       "      <td>0.119403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.122784</td>\n",
       "      <td>0.123670</td>\n",
       "      <td>0.118351</td>\n",
       "      <td>0.135195</td>\n",
       "      <td>0.132979</td>\n",
       "      <td>0.133865</td>\n",
       "      <td>0.121011</td>\n",
       "      <td>0.112145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.120676</td>\n",
       "      <td>0.131134</td>\n",
       "      <td>0.111022</td>\n",
       "      <td>0.123089</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.131134</td>\n",
       "      <td>0.131939</td>\n",
       "      <td>0.119067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.118046</td>\n",
       "      <td>0.116689</td>\n",
       "      <td>0.132972</td>\n",
       "      <td>0.127996</td>\n",
       "      <td>0.132067</td>\n",
       "      <td>0.130710</td>\n",
       "      <td>0.125283</td>\n",
       "      <td>0.116237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.137380</td>\n",
       "      <td>0.137380</td>\n",
       "      <td>0.118211</td>\n",
       "      <td>0.105431</td>\n",
       "      <td>0.150160</td>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.111821</td>\n",
       "      <td>0.105431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.109551</td>\n",
       "      <td>0.095506</td>\n",
       "      <td>0.132022</td>\n",
       "      <td>0.148876</td>\n",
       "      <td>0.103933</td>\n",
       "      <td>0.137640</td>\n",
       "      <td>0.143258</td>\n",
       "      <td>0.129213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.126943</td>\n",
       "      <td>0.107081</td>\n",
       "      <td>0.128670</td>\n",
       "      <td>0.120035</td>\n",
       "      <td>0.132988</td>\n",
       "      <td>0.136442</td>\n",
       "      <td>0.130397</td>\n",
       "      <td>0.117444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.128818</td>\n",
       "      <td>0.132802</td>\n",
       "      <td>0.115538</td>\n",
       "      <td>0.128818</td>\n",
       "      <td>0.131474</td>\n",
       "      <td>0.138114</td>\n",
       "      <td>0.111554</td>\n",
       "      <td>0.112882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>0.129178</td>\n",
       "      <td>0.132733</td>\n",
       "      <td>0.116852</td>\n",
       "      <td>0.125148</td>\n",
       "      <td>0.127281</td>\n",
       "      <td>0.123015</td>\n",
       "      <td>0.126570</td>\n",
       "      <td>0.119223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>0.125055</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>0.123164</td>\n",
       "      <td>0.125636</td>\n",
       "      <td>0.127963</td>\n",
       "      <td>0.123019</td>\n",
       "      <td>0.122437</td>\n",
       "      <td>0.125636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>0.126614</td>\n",
       "      <td>0.117953</td>\n",
       "      <td>0.131339</td>\n",
       "      <td>0.123622</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.129134</td>\n",
       "      <td>0.128661</td>\n",
       "      <td>0.122677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>0.123071</td>\n",
       "      <td>0.124148</td>\n",
       "      <td>0.133477</td>\n",
       "      <td>0.128095</td>\n",
       "      <td>0.122354</td>\n",
       "      <td>0.117869</td>\n",
       "      <td>0.133656</td>\n",
       "      <td>0.117330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.141414</td>\n",
       "      <td>0.101010</td>\n",
       "      <td>0.151515</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>0.128957</td>\n",
       "      <td>0.129888</td>\n",
       "      <td>0.123371</td>\n",
       "      <td>0.118715</td>\n",
       "      <td>0.121043</td>\n",
       "      <td>0.117784</td>\n",
       "      <td>0.123836</td>\n",
       "      <td>0.136406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>0.119658</td>\n",
       "      <td>0.129274</td>\n",
       "      <td>0.118590</td>\n",
       "      <td>0.120726</td>\n",
       "      <td>0.143162</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>0.135150</td>\n",
       "      <td>0.109509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>0.119617</td>\n",
       "      <td>0.172249</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.153110</td>\n",
       "      <td>0.138756</td>\n",
       "      <td>0.119617</td>\n",
       "      <td>0.100479</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.222221</td>\n",
       "      <td>0.158730</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.079366</td>\n",
       "      <td>0.031747</td>\n",
       "      <td>0.190475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>0.115468</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.124975</td>\n",
       "      <td>0.120222</td>\n",
       "      <td>0.125966</td>\n",
       "      <td>0.126164</td>\n",
       "      <td>0.133492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>0.120081</td>\n",
       "      <td>0.126942</td>\n",
       "      <td>0.125328</td>\n",
       "      <td>0.115237</td>\n",
       "      <td>0.123310</td>\n",
       "      <td>0.130373</td>\n",
       "      <td>0.130979</td>\n",
       "      <td>0.127750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0.123210</td>\n",
       "      <td>0.120071</td>\n",
       "      <td>0.124779</td>\n",
       "      <td>0.128115</td>\n",
       "      <td>0.126937</td>\n",
       "      <td>0.126349</td>\n",
       "      <td>0.132038</td>\n",
       "      <td>0.118501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>0.108225</td>\n",
       "      <td>0.114719</td>\n",
       "      <td>0.129870</td>\n",
       "      <td>0.134199</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.140693</td>\n",
       "      <td>0.116883</td>\n",
       "      <td>0.119048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>0.128448</td>\n",
       "      <td>0.130506</td>\n",
       "      <td>0.125154</td>\n",
       "      <td>0.127625</td>\n",
       "      <td>0.116509</td>\n",
       "      <td>0.126801</td>\n",
       "      <td>0.124743</td>\n",
       "      <td>0.120214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.123407</td>\n",
       "      <td>0.122055</td>\n",
       "      <td>0.120317</td>\n",
       "      <td>0.130939</td>\n",
       "      <td>0.131325</td>\n",
       "      <td>0.124372</td>\n",
       "      <td>0.129973</td>\n",
       "      <td>0.117613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>0.117374</td>\n",
       "      <td>0.128220</td>\n",
       "      <td>0.128607</td>\n",
       "      <td>0.126864</td>\n",
       "      <td>0.116599</td>\n",
       "      <td>0.129382</td>\n",
       "      <td>0.131513</td>\n",
       "      <td>0.121441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>0.126932</td>\n",
       "      <td>0.126045</td>\n",
       "      <td>0.132252</td>\n",
       "      <td>0.120091</td>\n",
       "      <td>0.127439</td>\n",
       "      <td>0.122118</td>\n",
       "      <td>0.122751</td>\n",
       "      <td>0.122371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>0.119172</td>\n",
       "      <td>0.124196</td>\n",
       "      <td>0.126608</td>\n",
       "      <td>0.130627</td>\n",
       "      <td>0.134646</td>\n",
       "      <td>0.117966</td>\n",
       "      <td>0.127010</td>\n",
       "      <td>0.119775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>0.121273</td>\n",
       "      <td>0.116790</td>\n",
       "      <td>0.120152</td>\n",
       "      <td>0.127550</td>\n",
       "      <td>0.128895</td>\n",
       "      <td>0.126429</td>\n",
       "      <td>0.132257</td>\n",
       "      <td>0.126653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>0.107317</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.138211</td>\n",
       "      <td>0.108943</td>\n",
       "      <td>0.113821</td>\n",
       "      <td>0.144715</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.120325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0.123883</td>\n",
       "      <td>0.127714</td>\n",
       "      <td>0.115581</td>\n",
       "      <td>0.119413</td>\n",
       "      <td>0.131865</td>\n",
       "      <td>0.128033</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.127075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>0.104129</td>\n",
       "      <td>0.140036</td>\n",
       "      <td>0.111311</td>\n",
       "      <td>0.122083</td>\n",
       "      <td>0.120287</td>\n",
       "      <td>0.127469</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>0.127469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0.129882</td>\n",
       "      <td>0.111717</td>\n",
       "      <td>0.115350</td>\n",
       "      <td>0.137148</td>\n",
       "      <td>0.125341</td>\n",
       "      <td>0.125341</td>\n",
       "      <td>0.123524</td>\n",
       "      <td>0.131698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0.146580</td>\n",
       "      <td>0.146580</td>\n",
       "      <td>0.127036</td>\n",
       "      <td>0.149837</td>\n",
       "      <td>0.133550</td>\n",
       "      <td>0.107492</td>\n",
       "      <td>0.084691</td>\n",
       "      <td>0.104235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.118483</td>\n",
       "      <td>0.123223</td>\n",
       "      <td>0.112796</td>\n",
       "      <td>0.129858</td>\n",
       "      <td>0.119905</td>\n",
       "      <td>0.129858</td>\n",
       "      <td>0.124645</td>\n",
       "      <td>0.141232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.124166</td>\n",
       "      <td>0.118648</td>\n",
       "      <td>0.123937</td>\n",
       "      <td>0.134054</td>\n",
       "      <td>0.129225</td>\n",
       "      <td>0.118418</td>\n",
       "      <td>0.124396</td>\n",
       "      <td>0.127156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.141104</td>\n",
       "      <td>0.119018</td>\n",
       "      <td>0.120859</td>\n",
       "      <td>0.127607</td>\n",
       "      <td>0.123926</td>\n",
       "      <td>0.118405</td>\n",
       "      <td>0.118405</td>\n",
       "      <td>0.130675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.115984</td>\n",
       "      <td>0.130604</td>\n",
       "      <td>0.118908</td>\n",
       "      <td>0.125731</td>\n",
       "      <td>0.148148</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.125731</td>\n",
       "      <td>0.115010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.147287</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.124031</td>\n",
       "      <td>0.105943</td>\n",
       "      <td>0.149871</td>\n",
       "      <td>0.087855</td>\n",
       "      <td>0.136951</td>\n",
       "      <td>0.136951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>0.131669</td>\n",
       "      <td>0.127128</td>\n",
       "      <td>0.114642</td>\n",
       "      <td>0.119183</td>\n",
       "      <td>0.120318</td>\n",
       "      <td>0.129398</td>\n",
       "      <td>0.128263</td>\n",
       "      <td>0.129398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6  \\\n",
       "0    0.110979  0.138425  0.113365  0.118138  0.126492  0.136038  0.132458   \n",
       "1    0.094286  0.115714  0.134286  0.148571  0.121429  0.130000  0.120000   \n",
       "2    0.106557  0.114754  0.119672  0.126230  0.114754  0.145902  0.126230   \n",
       "3    0.117560  0.136161  0.119792  0.136161  0.119792  0.125000  0.121280   \n",
       "4    0.113449  0.116194  0.110704  0.137237  0.139982  0.141812  0.127173   \n",
       "5    0.129252  0.125850  0.108844  0.119048  0.142857  0.113946  0.146258   \n",
       "6    0.118613  0.124088  0.144161  0.098540  0.124088  0.140511  0.116788   \n",
       "7    0.123214  0.118651  0.130754  0.128373  0.119246  0.132341  0.120238   \n",
       "8    0.118497  0.104046  0.121387  0.127168  0.144509  0.101156  0.135838   \n",
       "9    0.122476  0.109018  0.142665  0.113055  0.126514  0.118439  0.148048   \n",
       "10   0.222221  0.111111  0.069445  0.097223  0.152777  0.138889  0.069445   \n",
       "11   0.147343  0.101449  0.144927  0.115942  0.118358  0.140097  0.111111   \n",
       "12   0.105166  0.092251  0.140221  0.125461  0.132841  0.145756  0.136531   \n",
       "13   0.136364  0.122378  0.143357  0.125874  0.108392  0.136364  0.136364   \n",
       "14   0.119118  0.132353  0.142647  0.125000  0.125000  0.117647  0.132353   \n",
       "15   0.132143  0.125000  0.096429  0.125000  0.142857  0.130357  0.126786   \n",
       "16   0.116149  0.128238  0.118307  0.123489  0.133851  0.132988  0.129965   \n",
       "17   0.137313  0.125373  0.095522  0.143284  0.167164  0.089552  0.095522   \n",
       "18   0.124386  0.130933  0.116203  0.139116  0.130933  0.130933  0.096563   \n",
       "19   0.133333  0.098765  0.138272  0.153086  0.123457  0.133333  0.123457   \n",
       "20   0.130579  0.109091  0.117355  0.143802  0.135537  0.140496  0.132231   \n",
       "21   0.119018  0.123313  0.119939  0.135583  0.126687  0.118098  0.129755   \n",
       "22   0.175373  0.100746  0.100746  0.119403  0.078358  0.119403  0.186567   \n",
       "23   0.122784  0.123670  0.118351  0.135195  0.132979  0.133865  0.121011   \n",
       "24   0.120676  0.131134  0.111022  0.123089  0.131939  0.131134  0.131939   \n",
       "25   0.118046  0.116689  0.132972  0.127996  0.132067  0.130710  0.125283   \n",
       "26   0.137380  0.137380  0.118211  0.105431  0.150160  0.134185  0.111821   \n",
       "27   0.109551  0.095506  0.132022  0.148876  0.103933  0.137640  0.143258   \n",
       "28   0.126943  0.107081  0.128670  0.120035  0.132988  0.136442  0.130397   \n",
       "29   0.128818  0.132802  0.115538  0.128818  0.131474  0.138114  0.111554   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "170  0.129178  0.132733  0.116852  0.125148  0.127281  0.123015  0.126570   \n",
       "171  0.125055  0.127090  0.123164  0.125636  0.127963  0.123019  0.122437   \n",
       "172  0.126614  0.117953  0.131339  0.123622  0.120000  0.129134  0.128661   \n",
       "173  0.123071  0.124148  0.133477  0.128095  0.122354  0.117869  0.133656   \n",
       "174  0.101010  0.141414  0.141414  0.141414  0.101010  0.151515  0.111111   \n",
       "175  0.128957  0.129888  0.123371  0.118715  0.121043  0.117784  0.123836   \n",
       "176  0.119658  0.129274  0.118590  0.120726  0.143162  0.123932  0.135150   \n",
       "177  0.119617  0.172249  0.090909  0.153110  0.138756  0.119617  0.100479   \n",
       "178  0.142857  0.222221  0.158730  0.095238  0.079366  0.079366  0.031747   \n",
       "179  0.115468  0.131313  0.122400  0.124975  0.120222  0.125966  0.126164   \n",
       "180  0.120081  0.126942  0.125328  0.115237  0.123310  0.130373  0.130979   \n",
       "181  0.123210  0.120071  0.124779  0.128115  0.126937  0.126349  0.132038   \n",
       "182  0.108225  0.114719  0.129870  0.134199  0.136364  0.140693  0.116883   \n",
       "183  0.128448  0.130506  0.125154  0.127625  0.116509  0.126801  0.124743   \n",
       "184  0.123407  0.122055  0.120317  0.130939  0.131325  0.124372  0.129973   \n",
       "185  0.117374  0.128220  0.128607  0.126864  0.116599  0.129382  0.131513   \n",
       "186  0.126932  0.126045  0.132252  0.120091  0.127439  0.122118  0.122751   \n",
       "187  0.119172  0.124196  0.126608  0.130627  0.134646  0.117966  0.127010   \n",
       "188  0.121273  0.116790  0.120152  0.127550  0.128895  0.126429  0.132257   \n",
       "189  0.107317  0.133333  0.138211  0.108943  0.113821  0.144715  0.133333   \n",
       "190  0.123883  0.127714  0.115581  0.119413  0.131865  0.128033  0.126437   \n",
       "191  0.104129  0.140036  0.111311  0.122083  0.120287  0.127469  0.147217   \n",
       "192  0.129882  0.111717  0.115350  0.137148  0.125341  0.125341  0.123524   \n",
       "193  0.146580  0.146580  0.127036  0.149837  0.133550  0.107492  0.084691   \n",
       "194  0.118483  0.123223  0.112796  0.129858  0.119905  0.129858  0.124645   \n",
       "195  0.124166  0.118648  0.123937  0.134054  0.129225  0.118418  0.124396   \n",
       "196  0.141104  0.119018  0.120859  0.127607  0.123926  0.118405  0.118405   \n",
       "197  0.115984  0.130604  0.118908  0.125731  0.148148  0.119883  0.125731   \n",
       "198  0.147287  0.111111  0.124031  0.105943  0.149871  0.087855  0.136951   \n",
       "199  0.131669  0.127128  0.114642  0.119183  0.120318  0.129398  0.128263   \n",
       "\n",
       "            7  \n",
       "0    0.124105  \n",
       "1    0.135714  \n",
       "2    0.145902  \n",
       "3    0.124256  \n",
       "4    0.113449  \n",
       "5    0.113946  \n",
       "6    0.133212  \n",
       "7    0.127183  \n",
       "8    0.147399  \n",
       "9    0.119785  \n",
       "10   0.138889  \n",
       "11   0.120773  \n",
       "12   0.121771  \n",
       "13   0.090909  \n",
       "14   0.105882  \n",
       "15   0.121429  \n",
       "16   0.117012  \n",
       "17   0.146269  \n",
       "18   0.130933  \n",
       "19   0.096296  \n",
       "20   0.090909  \n",
       "21   0.127607  \n",
       "22   0.119403  \n",
       "23   0.112145  \n",
       "24   0.119067  \n",
       "25   0.116237  \n",
       "26   0.105431  \n",
       "27   0.129213  \n",
       "28   0.117444  \n",
       "29   0.112882  \n",
       "..        ...  \n",
       "170  0.119223  \n",
       "171  0.125636  \n",
       "172  0.122677  \n",
       "173  0.117330  \n",
       "174  0.111111  \n",
       "175  0.136406  \n",
       "176  0.109509  \n",
       "177  0.105263  \n",
       "178  0.190475  \n",
       "179  0.133492  \n",
       "180  0.127750  \n",
       "181  0.118501  \n",
       "182  0.119048  \n",
       "183  0.120214  \n",
       "184  0.117613  \n",
       "185  0.121441  \n",
       "186  0.122371  \n",
       "187  0.119775  \n",
       "188  0.126653  \n",
       "189  0.120325  \n",
       "190  0.127075  \n",
       "191  0.127469  \n",
       "192  0.131698  \n",
       "193  0.104235  \n",
       "194  0.141232  \n",
       "195  0.127156  \n",
       "196  0.130675  \n",
       "197  0.115010  \n",
       "198  0.136951  \n",
       "199  0.129398  \n",
       "\n",
       "[200 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_frame1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-fdf6dced00a2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msampler2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLdaSampler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_frame1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mntopics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest_lda2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msampler2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunfort\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mmaxiter\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m12000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mburnin\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_frame1' is not defined"
     ]
    }
   ],
   "source": [
    "sampler2 = LdaSampler(data = train_frame1, ntopics = 20, alpha = .1, beta = .1)\n",
    "test_lda2 = sampler2.runfort( maxiter =12000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler3 = LdaSampler(data = train_frame1, ntopics = 5, alpha = .1, beta = .1)\n",
    "test_lda3 = sampler3.runfort( maxiter =12000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler4 = LdaSampler(data = train_frame1, ntopics = 30, alpha = .1, beta = .1)\n",
    "test_lda4 = sampler4.runfort( maxiter =12000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_lda3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Grab made up data\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "N_TOPICS = 10\n",
    "DOCUMENT_LENGTH = 100\n",
    "FOLDER = \"topicimg\"\n",
    "\n",
    "def vertical_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a vertical bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[:, topic_index] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def horizontal_topic(width, topic_index, document_length):\n",
    "    \"\"\"\n",
    "    Generate a topic whose words form a horizontal bar.\n",
    "    \"\"\"\n",
    "    m = np.zeros((width, width))\n",
    "    m[topic_index, :] = int(document_length / width)\n",
    "    return m.flatten()\n",
    "\n",
    "def save_document_image(filename, doc, zoom=2):\n",
    "    \"\"\"\n",
    "    Save document as an image.\n",
    "    doc must be a square matrix\n",
    "    \"\"\"\n",
    "    height, width = doc.shape\n",
    "    zoom = np.ones((width*zoom, width*zoom))\n",
    "    # imsave scales pixels between 0 and 255 automatically\n",
    "    sp.misc.imsave(filename, np.kron(doc, zoom))\n",
    "\n",
    "def gen_word_distribution(ntopics, document_length):\n",
    "    \"\"\"\n",
    "    Generate a word distribution for each of the ntopics.\n",
    "    \"\"\"\n",
    "    width = ntopics / 2\n",
    "    vsize = width ** 2\n",
    "    m = np.zeros((ntopics, vsize))\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k,:] = vertical_topic(width, k, document_length)\n",
    "\n",
    "    for k in range(width):\n",
    "        m[k+width,:] = horizontal_topic(width, k, document_length)\n",
    "\n",
    "    m /= m.sum(axis=1)[:, np.newaxis] # turn counts into probabilities\n",
    "\n",
    "    return m\n",
    "\n",
    "def gen_document(word_dist, ntopics, vsize, length=DOCUMENT_LENGTH, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Generate a document:\n",
    "    1) Sample topic proportions from the Dirichlet distribution.\n",
    "    2) Sample a topic index from the Multinomial with the topic\n",
    "       proportions from 1).\n",
    "    3) Sample a word from the Multinomial corresponding to the topic\n",
    "       index from 2).\n",
    "    4) Go to 2) if need another word.\n",
    "    \"\"\"\n",
    "    theta = np.random.mtrand.dirichlet([alpha] * ntopics)\n",
    "    v = np.zeros(vsize)\n",
    "    for n in range(length):\n",
    "        z = index_sample(theta)\n",
    "        w = index_sample(word_dist[z,:])\n",
    "        v[w] += 1\n",
    "    return v\n",
    "\n",
    "def gen_documents(word_dist, ntopics, vsize, n=500):\n",
    "    \"\"\"\n",
    "    Generate a document-term matrix.\n",
    "    \"\"\"\n",
    "    m = np.zeros((n, vsize))\n",
    "    for i in xrange(n):\n",
    "        m[i, :] = gen_document(word_dist, ntopics, vsize)\n",
    "    \n",
    "    return m\n",
    "\n",
    "if os.path.exists(FOLDER):\n",
    "    shutil.rmtree(FOLDER)\n",
    "os.mkdir(FOLDER)\n",
    "\n",
    "width = N_TOPICS / 2\n",
    "vocab_size = width ** 2\n",
    "word_dist = gen_word_distribution(N_TOPICS, DOCUMENT_LENGTH)\n",
    "matrix = gen_documents(word_dist, N_TOPICS, vocab_size)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
