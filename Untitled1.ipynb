{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gibbssampler(matrix,nzw,nzm,nz,nm,p_z,topics,alpha,beta,lik,[ntopics,max_iter,m,n,top_size])\n",
      "\n",
      "Wrapper for ``gibbssampler``.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "matrix : input rank-2 array('q') with bounds (n,m)\n",
      "nzw : input rank-2 array('q') with bounds (ntopics,n)\n",
      "nzm : input rank-2 array('q') with bounds (ntopics,m)\n",
      "nz : input rank-1 array('q') with bounds (ntopics)\n",
      "nm : input rank-1 array('q') with bounds (m)\n",
      "p_z : input rank-1 array('d') with bounds (ntopics)\n",
      "topics : input rank-1 array('q') with bounds (top_size)\n",
      "alpha : input float\n",
      "beta : input float\n",
      "lik : in/output rank-1 array('d') with bounds (max_iter)\n",
      "\n",
      "Other Parameters\n",
      "----------------\n",
      "ntopics : input long, optional\n",
      "    Default: shape(nzw,0)\n",
      "max_iter : input long, optional\n",
      "    Default: len(lik)\n",
      "m : input long, optional\n",
      "    Default: shape(matrix,1)\n",
      "n : input long, optional\n",
      "    Default: shape(matrix,0)\n",
      "top_size : input long, optional\n",
      "    Default: len(topics)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('./fortran')\n",
    "import gibbsSampler6th\n",
    "print gibbsSampler6th.gibbs_sampler.gibbssampler.__doc__\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "random.seed(1234)\n",
    "#\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n",
    "def index_sample(p):\n",
    "    \"\"\"\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    \"\"\"\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    \"\"\"\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "\n",
    "class LdaSampler(object):\n",
    "\n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        #if not isinstance(alpha, float):\n",
    "        #    raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "\n",
    "        #if not isinstance(beta, float):\n",
    "        #    raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "\n",
    "        #if not isinstance(ntopics, int):\n",
    "        #    raise Exception(\" The number of topics must be an integer\")\n",
    "\n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "\n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "\n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "\n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "\n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        \"\"\"\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "\n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = []\n",
    "        self.logL = []\n",
    "        \n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for n in xrange(vsize):\n",
    "                if self.matrix[m,n] == 0:\n",
    "                    continue\n",
    "                for w in xrange(self.matrix[m,n]):\n",
    "                # Initialize a random topic for each word\n",
    "                    z = np.random.randint(self.ntopics)\n",
    "                    self.topics.append(z)\n",
    "                    self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,n] += 1\n",
    "                    self.NZ[z] += 1\n",
    "                # Keep document, iterator for word, word index, and assignment\n",
    "                #self.topics.append([i,w,z])\n",
    "        \n",
    "        self.topics = np.vstack(self.topics)\n",
    "    def _conditional_distribution(self, m, n):\n",
    "        \"\"\"\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "\n",
    "        Output: p_z: An array size(1 X ntopics) containing\n",
    "                  probabilities for topics of word\n",
    "        \n",
    "        The formula is:\n",
    "        ((n_{k,-i}^(t) + \\beta_t)/\\sum_{t=1}^V(n_{k,-i}^(t) + \\beta_t)) *\n",
    "        ((n_{m,-i}^(t) + \\alpha_k)/(\\sum_{k=1}^K(n_m^k + \\alpha_k) - 1))\n",
    "        \"\"\"\n",
    "        vsize = self.matrix[m,:].nonzero()[0].size\n",
    "        p_z = np.zeros(self.ntopics)\n",
    "        for ii in xrange(self.ntopics):\n",
    "            p_z[ii] = (self.NZM[m,ii] + self.alpha) \\\n",
    "            *(self.NZW[ii,n] + self.beta) \\\n",
    "            / (self.NZ[ii] + vsize * self.beta)\n",
    "        \n",
    "        p_z /= np.sum(p_z)\n",
    "\n",
    "        return p_z\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        \"\"\"\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.matrix[m,:].nonzero()[0].size\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "\n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "\n",
    "        return lik\n",
    "\n",
    "    def phi_theta(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding\n",
    "            [0] phi: Probability of topic by word\n",
    "            [1] theta: Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 0)[np.newaxis,:]\n",
    "\n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:,np.newaxis ]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "\n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        n_docs, vsize = self.matrix.shape\n",
    "        topics2 = self.topics\n",
    "\n",
    "\n",
    "        for iteration in xrange(maxiter + 2):\n",
    "            # Idea: After each iteration we now want to\n",
    "            # make assignments relative to the newly generated topics\n",
    "            if iteration > 1:\n",
    "                self.topics = topics2\n",
    "            for m in xrange(n_docs):\n",
    "                for n in xrange(vsize):\n",
    "                    if self.matrix[m,n] == 0:\n",
    "                        continue\n",
    "                    for w in xrange(self.matrix[m,n]):\n",
    "                        \n",
    "                        z = self.topics[m,n]\n",
    "                    \n",
    "                        self.NZM[m,z] -= 1\n",
    "                        self.NM[m] -= 1\n",
    "                        self.NZW[z,n] -= 1\n",
    "                        self.NZ[z] -= 1\n",
    "\n",
    "                        p_z = self._conditional_distribution(m,n)\n",
    "                        # Choosing a random topic row\n",
    "                        ind_z = np.random.randint(self.ntopics)\n",
    "                    \n",
    "                        # Sampling random topic\n",
    "                        z = index_sample(p_z)\n",
    "                        \n",
    "                        #Self.topics needs to change after we iterate over this word\n",
    "                        # Otherwise at each iteration we subtract one from that space w*n times\n",
    "                        # giving us a negative number\n",
    "                        topics2[m,n] = z\n",
    "\n",
    "                        self.NZM[m,z] += 1\n",
    "                        self.NM[m] += 1\n",
    "                        self.NZW[z,n] += 1\n",
    "                        self.NZ[z] += 1\n",
    "\n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "                \n",
    "    def runfort(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        M,N = self.matrix.shape\n",
    "\n",
    "        p_z = np.zeros(self.ntopics)\n",
    "        p_z += 1./self.ntopics\n",
    "        \n",
    " \n",
    "        # Make everything fortan contiguous\n",
    "        p_z=p_z.flatten() # Flatten array (Make 1-D)\n",
    "        p_z=p_z.reshape(self.ntopics, order='F')\n",
    "        run_matrix = np.array(self.matrix.transpose(),order='F')\n",
    "        run_NZM = np.array(self.NZM.transpose(),order='F')\n",
    "        run_NZW = np.array(self.NZW,order='F')\n",
    "        run_NZ = np.array(self.NZ,order='F')\n",
    "        run_NM = np.array(self.NM,order='F')\n",
    "        # index starts at 1 in fortran\n",
    "        run_topics = np.array(self.topics,order='F') + 1\n",
    "        run_topics2 = run_topics\n",
    "        \n",
    "        \n",
    "        topics2 = self.topics.transpose()\n",
    "        loglik = np.zeros(maxiter)\n",
    "        \n",
    "        gibbsSampler6th.gibbs_sampler.gibbssampler(matrix = run_matrix,\n",
    "                                                nzw = run_NZW,\n",
    "                                                nzm = run_NZM,\n",
    "                                                nz = run_NZ,\n",
    "                                                nm = run_NM,\n",
    "                                                max_iter = maxiter,\n",
    "                                                p_z = p_z,\n",
    "                                                m = M,\n",
    "                                                n = N,\n",
    "                                                topics = run_topics,\n",
    "                                                alpha = self.alpha,\n",
    "                                                beta = self.beta,\n",
    "                                                lik = loglik)\n",
    "                                                \n",
    "        \n",
    "        self.NZM = run_NZM.transpose()\n",
    "        self.matrix = run_matrix.transpose()\n",
    "        self.NZW = run_NZW\n",
    "        self.NZ = run_NZ\n",
    "        self.NM = run_NM\n",
    "\n",
    "        #if iteration > burnin:\n",
    "        return self.phi_theta(),loglik\n",
    "                \n",
    "                \n",
    "\n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "\n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding\n",
    "                [0] Probability of topic by word\n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        \n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "            self.logL.append(self.loglikelihood())\n",
    "        return self.phi_theta(), self.logL\n",
    "\n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ\n",
    "        self.logL = self.logL\n",
    "        \n",
    "        \n",
    "#\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "p=.2\n",
    "# sample files for train\n",
    "gen_sample = np.array(sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))))\n",
    "rand_sample = [ allfiles[i] for i in gen_sample ]\n",
    "#\n",
    "# take rest for test\n",
    "rand_sample2 = []\n",
    "for i in xrange(len(allfiles)):\n",
    "    if i not in gen_sample:\n",
    "        rand_sample2.append(allfiles[i])\n",
    "#\n",
    "# train data\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "#\n",
    "# test data\n",
    "np_array_list_test = []\n",
    "for file_ in rand_sample2:\n",
    "    df = pd.read_csv(file_, index_col = None, header = 0)\n",
    "    df['source'] = file_\n",
    "    np_array_list_test.append(df.as_matrix())\n",
    "    \n",
    "#\n",
    "# train data frame\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "train_frame = pd.DataFrame(comb_np_array)\n",
    "train_frame.columns = ['words','count','source']\n",
    "subless = (train_frame['words'].str.len() > 2)\n",
    "submore = (train_frame['words'].str.len() < 20)\n",
    "train_frame = train_frame.loc[subless]\n",
    "train_frame = train_frame.loc[submore]\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "train_frame = train_frame.fillna(value = 0)\n",
    "train_frame = train_frame.loc[:, (train_frame.sum(axis = 0) > 10)]\n",
    "#\n",
    "\n",
    "# test data frame\n",
    "comb_np_array_test = np.vstack(np_array_list_test)\n",
    "test_frame = pd.DataFrame(comb_np_array_test)\n",
    "test_frame.columns = ['words','count','source']\n",
    "test_frame = test_frame.fillna(value=0)\n",
    "test_frame = test_frame.pivot(index = 'source', columns = 'words', values = 'count')\n",
    "test_frame = test_frame.fillna(value = 0)\n",
    "\n",
    "train_frame1 = train_frame.values.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sampler2 = LdaSampler(data = train_frame1, ntopics = 5, alpha = .0001, beta = .0001)\n",
    "test_lda2 = sampler2.runfort( maxiter =12000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_lda3 = sampler2.runfort( maxiter =3000, burnin = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihood = test_lda2[1]\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(likelihood)\n",
    "plt.ylabel('Negative Log Likelihood')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3569)\n"
     ]
    }
   ],
   "source": [
    "phi_theta = test_lda2[0]\n",
    "phi = pd.DataFrame(data=phi_theta[0], columns = train_frame.columns)\n",
    "theta = pd.DataFrame(data=phi_theta[1])\n",
    "print phi.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([[  2.10526039e-01,   1.33334074e-01,   2.82607798e-01, ...,\n",
       "            1.25001562e-01,   2.11055249e-01,   9.09140494e-02],\n",
       "         [  2.63156233e-01,   3.33331852e-01,   1.52174433e-01, ...,\n",
       "            1.66667361e-01,   2.41205927e-01,   9.09049589e-06],\n",
       "         [  2.10526039e-01,   3.33331852e-01,   2.39130009e-01, ...,\n",
       "            1.66667361e-01,   1.80904571e-01,   9.09049589e-06],\n",
       "         [  2.10526039e-01,   8.88901234e-02,   1.30435539e-01, ...,\n",
       "            2.08333160e-01,   1.65829232e-01,   6.36343803e-01],\n",
       "         [  1.05265651e-01,   1.11112099e-01,   1.95652221e-01, ...,\n",
       "            3.33330556e-01,   2.01005023e-01,   2.72723967e-01]]),\n",
       "  array([[ 0.21479713,  0.17303104,  0.24224341,  0.19093079,  0.17899763],\n",
       "         [ 0.19714286,  0.18142858,  0.21571427,  0.19714286,  0.20857142],\n",
       "         [ 0.20491803,  0.21639343,  0.19508197,  0.20983606,  0.17377051],\n",
       "         [ 0.21205357,  0.20535714,  0.20238095,  0.19196429,  0.18824405],\n",
       "         [ 0.21134492,  0.1957914 ,  0.19853614,  0.19121684,  0.2031107 ],\n",
       "         [ 0.21088434,  0.20408163,  0.19217688,  0.21258502,  0.18027213],\n",
       "         [ 0.20620437,  0.20255474,  0.18978103,  0.22810216,  0.17335769],\n",
       "         [ 0.20099206,  0.19880952,  0.20694444,  0.19206349,  0.20119048],\n",
       "         [ 0.1965318 ,  0.19364163,  0.21676298,  0.20231214,  0.19075146],\n",
       "         [ 0.17765816,  0.22207266,  0.19784657,  0.18707942,  0.21534319],\n",
       "         [ 0.23611086,  0.1666669 ,  0.1666669 ,  0.20833328,  0.22222207],\n",
       "         [ 0.19323672,  0.17149762,  0.19565218,  0.21014492,  0.22946856],\n",
       "         [ 0.2435424 ,  0.17343176,  0.19372694,  0.18819189,  0.20110701],\n",
       "         [ 0.18181821,  0.21328669,  0.20979019,  0.18881121,  0.2062937 ],\n",
       "         [ 0.19411765,  0.21470587,  0.20735294,  0.18676472,  0.19705883],\n",
       "         [ 0.2142857 ,  0.2       ,  0.18928572,  0.21071428,  0.1857143 ],\n",
       "         [ 0.19818653,  0.19818653,  0.20034542,  0.20811744,  0.19516408],\n",
       "         [ 0.21492535,  0.20597014,  0.18208958,  0.2298507 ,  0.16716423],\n",
       "         [ 0.18657939,  0.18821605,  0.22258591,  0.20785597,  0.19476269],\n",
       "         [ 0.15061734,  0.21728393,  0.20987653,  0.21234566,  0.20987653],\n",
       "         [ 0.2       ,  0.2       ,  0.20330578,  0.15371905,  0.24297517],\n",
       "         [ 0.19907975,  0.19355828,  0.20705521,  0.19202454,  0.20828221],\n",
       "         [ 0.16791051,  0.22761189,  0.25373124,  0.17537318,  0.17537318],\n",
       "         [ 0.19326241,  0.20390071,  0.19636525,  0.19946809,  0.20700354],\n",
       "         [ 0.22767497,  0.19147225,  0.19710378,  0.20514883,  0.17860017],\n",
       "         [ 0.20352781,  0.20578923,  0.19855269,  0.19855269,  0.19357757],\n",
       "         [ 0.19808307,  0.18210865,  0.21725237,  0.18530354,  0.21725237],\n",
       "         [ 0.21067414,  0.17696632,  0.17134835,  0.20224719,  0.23876399],\n",
       "         [ 0.19516408,  0.21934369,  0.19343696,  0.19343696,  0.19861831],\n",
       "         [ 0.20717131,  0.19389111,  0.21115537,  0.16998674,  0.21779547],\n",
       "         [ 0.17391333,  0.10869664,  0.2608689 ,  0.21739112,  0.23913001],\n",
       "         [ 0.22702695,  0.17837844,  0.17837844,  0.22702695,  0.18918922],\n",
       "         [ 0.18301888,  0.21886791,  0.17169814,  0.22830186,  0.19811321],\n",
       "         [ 0.19147497,  0.20500676,  0.2158322 ,  0.19959405,  0.18809202],\n",
       "         [ 0.23021581,  0.19544365,  0.18585133,  0.19304557,  0.19544365],\n",
       "         [ 0.2075471 ,  0.18867935,  0.18867935,  0.22641485,  0.18867935],\n",
       "         [ 0.20251836,  0.19937041,  0.20636586,  0.19797132,  0.19377405],\n",
       "         [ 0.19424463,  0.18705041,  0.17985619,  0.21582728,  0.2230215 ],\n",
       "         [ 0.18989281,  0.21745787,  0.18223585,  0.19295559,  0.21745787],\n",
       "         [ 0.21556882,  0.15568876,  0.24550885,  0.21556882,  0.16766477],\n",
       "         [ 0.20255183,  0.18660288,  0.19617225,  0.20893141,  0.20574162],\n",
       "         [ 0.19763475,  0.2037753 ,  0.19649761,  0.19604276,  0.20604958],\n",
       "         [ 0.18720381,  0.18957347,  0.19905213,  0.21800946,  0.20616113],\n",
       "         [ 0.20976692,  0.21124676,  0.18608953,  0.19385868,  0.19903811],\n",
       "         [ 0.21008402,  0.18907564,  0.18907564,  0.22899157,  0.18277313],\n",
       "         [ 0.20697674,  0.18139537,  0.20697674,  0.19302326,  0.21162789],\n",
       "         [ 0.16864612,  0.216152  ,  0.23752965,  0.21377671,  0.16389553],\n",
       "         [ 0.21224489,  0.19795918,  0.19489796,  0.17755103,  0.21734693],\n",
       "         [ 0.22894165,  0.14902813,  0.21814253,  0.17926568,  0.224622  ],\n",
       "         [ 0.20059752,  0.20358515,  0.20529236,  0.21340162,  0.17712335],\n",
       "         [ 0.19210978,  0.19039452,  0.19210978,  0.20754716,  0.21783875],\n",
       "         [ 0.20422535,  0.21361501,  0.20657276,  0.18779344,  0.18779344],\n",
       "         [ 0.1846155 ,  0.27692249,  0.1846155 ,  0.2       ,  0.15384651],\n",
       "         [ 0.18313071,  0.1762918 ,  0.20896656,  0.2112462 ,  0.22036473],\n",
       "         [ 0.1923077 ,  0.19899666,  0.19565218,  0.19565218,  0.21739129],\n",
       "         [ 0.20655354,  0.18724401,  0.19016969,  0.21181977,  0.20421299],\n",
       "         [ 0.19270834,  0.16927087,  0.16927087,  0.23437496,  0.23437496],\n",
       "         [ 0.20017953,  0.20466786,  0.20736086,  0.19120288,  0.19658887],\n",
       "         [ 0.20311841,  0.19764012,  0.20311841,  0.19974716,  0.1963759 ],\n",
       "         [ 0.20201038,  0.19844358,  0.20265888,  0.2039559 ,  0.19293126],\n",
       "         [ 0.19721116,  0.17529882,  0.18924303,  0.2320717 ,  0.2061753 ],\n",
       "         [ 0.21455223,  0.20522388,  0.18376866,  0.19216418,  0.20429104],\n",
       "         [ 0.20627615,  0.20125523,  0.19205021,  0.19288703,  0.20753138],\n",
       "         [ 0.19444445,  0.20019157,  0.21264367,  0.19444445,  0.19827586],\n",
       "         [ 0.2       ,  0.13333389,  0.18333347,  0.23333306,  0.24999958],\n",
       "         [ 0.18950439,  0.19241984,  0.20408163,  0.19241984,  0.22157431],\n",
       "         [ 0.19660194,  0.20226537,  0.22653721,  0.1828479 ,  0.19174758],\n",
       "         [ 0.22240801,  0.19899666,  0.20234114,  0.17056859,  0.20568561],\n",
       "         [ 0.21938773,  0.19897959,  0.18112247,  0.21683671,  0.18367349],\n",
       "         [ 0.21879193,  0.20939597,  0.18657719,  0.2       ,  0.18523491],\n",
       "         [ 0.2054409 ,  0.20356473,  0.18480301,  0.20731707,  0.1988743 ],\n",
       "         [ 0.2076923 ,  0.18846155,  0.18653847,  0.2307692 ,  0.18653847],\n",
       "         [ 0.21978011,  0.12087956,  0.14285746,  0.25274696,  0.26373591],\n",
       "         [ 0.20564872,  0.18534864,  0.20388349,  0.21006178,  0.19505737],\n",
       "         [ 0.18421058,  0.15789488,  0.1907895 ,  0.24999984,  0.21710521],\n",
       "         [ 0.22047241,  0.16272971,  0.1968504 ,  0.20734907,  0.21259841],\n",
       "         [ 0.1954571 ,  0.21172182,  0.20106562,  0.2019069 ,  0.18984857],\n",
       "         [ 0.19223259,  0.20091126,  0.19223259,  0.2130614 ,  0.20156216],\n",
       "         [ 0.20231395,  0.21794872,  0.19543465,  0.17886179,  0.2054409 ],\n",
       "         [ 0.2051282 ,  0.17708334,  0.2051282 ,  0.2051282 ,  0.20753205],\n",
       "         [ 0.20280102,  0.20316479,  0.18825027,  0.19861768,  0.20716624],\n",
       "         [ 0.18677495,  0.23201854,  0.21113688,  0.18793504,  0.18213458],\n",
       "         [ 0.20136519,  0.19453926,  0.20136519,  0.19453926,  0.20819111],\n",
       "         [ 0.2120658 ,  0.21023765,  0.2120658 ,  0.17915907,  0.18647168],\n",
       "         [ 0.2052139 ,  0.18181819,  0.19986631,  0.2078877 ,  0.2052139 ],\n",
       "         [ 0.18507158,  0.22290387,  0.18813907,  0.20245399,  0.20143149],\n",
       "         [ 0.20204978,  0.19912152,  0.1976574 ,  0.19033676,  0.21083455],\n",
       "         [ 0.19529883,  0.20330082,  0.19179795,  0.21355339,  0.19604901],\n",
       "         [ 0.20458801,  0.18867041,  0.19803371,  0.19382023,  0.21488764],\n",
       "         [ 0.19031144,  0.19377164,  0.22145325,  0.19031144,  0.20415224],\n",
       "         [ 0.20916666,  0.20916666,  0.19833333,  0.18208334,  0.20125   ],\n",
       "         [ 0.19835841,  0.19699043,  0.23529409,  0.17783859,  0.19151847],\n",
       "         [ 0.16888891,  0.22962961,  0.21037036,  0.19703704,  0.19407408],\n",
       "         [ 0.20072993,  0.18978104,  0.20802918,  0.20072993,  0.20072993],\n",
       "         [ 0.20314548,  0.21428571,  0.20052425,  0.19790302,  0.18414155],\n",
       "         [ 0.1811205 ,  0.2095165 ,  0.19109747,  0.21719109,  0.20107444],\n",
       "         [ 0.19041096,  0.21849315,  0.1989726 ,  0.19657534,  0.19554795],\n",
       "         [ 0.19941776,  0.20291121,  0.20262009,  0.19505095,  0.2       ],\n",
       "         [ 0.2       ,  0.1959596 ,  0.2040404 ,  0.2121212 ,  0.1878788 ],\n",
       "         [ 0.20246011,  0.1924867 ,  0.19581117,  0.20013298,  0.20910904],\n",
       "         [ 0.20502901,  0.20580271,  0.19497099,  0.20348162,  0.19071567],\n",
       "         [ 0.18627453,  0.19852941,  0.22058821,  0.19607844,  0.19852941],\n",
       "         [ 0.20854484,  0.19416244,  0.2036802 ,  0.19141286,  0.20219966],\n",
       "         [ 0.13333444,  0.26666556,  0.16666722,  0.23333278,  0.2       ],\n",
       "         [ 0.1917939 ,  0.20229008,  0.20706107,  0.19847328,  0.20038168],\n",
       "         [ 0.19765984,  0.1930631 ,  0.20225658,  0.20518178,  0.2018387 ],\n",
       "         [ 0.20008681,  0.18576389,  0.21397569,  0.20138889,  0.19878472],\n",
       "         [ 0.19291339,  0.20669291,  0.1875    ,  0.20964567,  0.20324803],\n",
       "         [ 0.18983053,  0.18305088,  0.1728814 ,  0.25762702,  0.19661018],\n",
       "         [ 0.20622914,  0.1948832 ,  0.19666296,  0.20422692,  0.19799778],\n",
       "         [ 0.21505374,  0.18996418,  0.16129039,  0.21505374,  0.21863796],\n",
       "         [ 0.203125  ,  0.20082721,  0.19577206,  0.20818015,  0.19209559],\n",
       "         [ 0.20892495,  0.20791075,  0.18204869,  0.18762678,  0.21348884],\n",
       "         [ 0.16734701,  0.20408162,  0.19591838,  0.20408162,  0.22857137],\n",
       "         [ 0.22321083,  0.18955513,  0.20038685,  0.18529981,  0.20154739],\n",
       "         [ 0.1876047 ,  0.21021775,  0.20770519,  0.18927974,  0.20519263],\n",
       "         [ 0.22558013,  0.19427955,  0.19643821,  0.19050189,  0.19320022],\n",
       "         [ 0.22608692,  0.20869564,  0.19420291,  0.17681163,  0.19420291],\n",
       "         [ 0.19387755,  0.20535714,  0.19617347,  0.20612245,  0.19846939],\n",
       "         [ 0.20165538,  0.20240782,  0.19789315,  0.19638826,  0.20165538],\n",
       "         [ 0.2413786 ,  0.20689643,  0.31034293,  0.1379321 ,  0.10344994],\n",
       "         [ 0.1808511 ,  0.22340421,  0.22340421,  0.17021282,  0.20212766],\n",
       "         [ 0.1972973 ,  0.20405405,  0.18108109,  0.22027026,  0.1972973 ],\n",
       "         [ 0.18556701,  0.19716495,  0.20670103,  0.20309278,  0.20747423],\n",
       "         [ 0.18119892,  0.21662124,  0.22070843,  0.17302454,  0.20844686],\n",
       "         [ 0.20667414,  0.19573752,  0.19770051,  0.21172182,  0.18816601],\n",
       "         [ 0.19599109,  0.1952487 ,  0.18708241,  0.21603563,  0.20564217],\n",
       "         [ 0.21249999,  0.21136363,  0.1909091 ,  0.19431819,  0.1909091 ],\n",
       "         [ 0.1872428 ,  0.20061728,  0.20884773,  0.19958848,  0.2037037 ],\n",
       "         [ 0.2047244 ,  0.18897639,  0.17060371,  0.21259841,  0.22309708],\n",
       "         [ 0.21039604,  0.1927157 ,  0.20615276,  0.19625177,  0.19448374],\n",
       "         [ 0.23489927,  0.18456378,  0.18456378,  0.17114099,  0.22483217],\n",
       "         [ 0.21513943,  0.17662684,  0.20849933,  0.20584329,  0.19389111],\n",
       "         [ 0.2095238 ,  0.18690477,  0.20119048,  0.19285715,  0.2095238 ],\n",
       "         [ 0.20693277,  0.22373948,  0.20063025,  0.18277312,  0.18592438],\n",
       "         [ 0.20338982,  0.23163833,  0.24293773,  0.124294  ,  0.19774012],\n",
       "         [ 0.2061706 ,  0.18911071,  0.21270417,  0.19092559,  0.20108893],\n",
       "         [ 0.21140702,  0.19184539,  0.18878152,  0.20527928,  0.20268678],\n",
       "         [ 0.19597684,  0.19963426,  0.21395916,  0.18683328,  0.20359646],\n",
       "         [ 0.15737712,  0.18688527,  0.17704922,  0.2196721 ,  0.2590163 ],\n",
       "         [ 0.17272731,  0.2060606 ,  0.2090909 ,  0.22727269,  0.18484851],\n",
       "         [ 0.1752137 ,  0.1837607 ,  0.2307692 ,  0.19871795,  0.21153845],\n",
       "         [ 0.21959458,  0.18693694,  0.19256757,  0.20495495,  0.19594595],\n",
       "         [ 0.2       ,  0.18846156,  0.16923083,  0.23076917,  0.21153844],\n",
       "         [ 0.20306905,  0.1945439 ,  0.2056266 ,  0.19880648,  0.19795396],\n",
       "         [ 0.20416362,  0.19138057,  0.19503287,  0.20306793,  0.206355  ],\n",
       "         [ 0.19813717,  0.20406435,  0.19263336,  0.20448772,  0.20067739],\n",
       "         [ 0.1891892 ,  0.21621619,  0.2       ,  0.2135135 ,  0.18108111],\n",
       "         [ 0.19474836,  0.190372  ,  0.214442  ,  0.20568927,  0.19474836],\n",
       "         [ 0.19116883,  0.20744589,  0.20069264,  0.20017316,  0.20051948],\n",
       "         [ 0.21400775,  0.21400775,  0.19844358,  0.1906615 ,  0.18287941],\n",
       "         [ 0.20234205,  0.19934641,  0.19880174,  0.19172113,  0.20778867],\n",
       "         [ 0.207021  ,  0.18832021,  0.2007874 ,  0.19914698,  0.20472441],\n",
       "         [ 0.20495495,  0.21396395,  0.20045045,  0.18693695,  0.1936937 ],\n",
       "         [ 0.18345974,  0.21054532,  0.18418202,  0.20440592,  0.217407  ],\n",
       "         [ 0.21325966,  0.20165746,  0.19005525,  0.20110497,  0.19392265],\n",
       "         [ 0.20028715,  0.20100503,  0.20004786,  0.20315865,  0.19550132],\n",
       "         [ 0.19717573,  0.20658996,  0.18933055,  0.20449791,  0.20240586],\n",
       "         [ 0.2082153 ,  0.19546742,  0.20077904,  0.20467422,  0.19086402],\n",
       "         [ 0.19965202,  0.20269682,  0.20595911,  0.19704219,  0.19464985],\n",
       "         [ 0.21752737,  0.19874804,  0.18153366,  0.20813771,  0.19405321],\n",
       "         [ 0.1797753 ,  0.16479404,  0.21348313,  0.22284642,  0.21910111],\n",
       "         [ 0.20881166,  0.18895439,  0.1898852 ,  0.20167546,  0.21067328],\n",
       "         [ 0.19851334,  0.19632707,  0.19239178,  0.20507215,  0.20769567],\n",
       "         [ 0.2174721 ,  0.18029742,  0.2118959 ,  0.18029742,  0.21003717],\n",
       "         [ 0.2054741 ,  0.18533724,  0.19843597,  0.20645161,  0.20430107],\n",
       "         [ 0.22857133,  0.21428566,  0.16428584,  0.16428584,  0.22857133],\n",
       "         [ 0.22009568,  0.20382775,  0.21435406,  0.18660288,  0.17511963],\n",
       "         [ 0.20454545,  0.1783217 ,  0.2202797 ,  0.19405595,  0.2027972 ],\n",
       "         [ 0.2272726 ,  0.2272726 ,  0.29090868,  0.16363653,  0.09090959],\n",
       "         [ 0.19506992,  0.21047641,  0.19056649,  0.19578099,  0.20810619],\n",
       "         [ 0.20575832,  0.19514323,  0.19267122,  0.20750327,  0.19892395],\n",
       "         [ 0.20362205,  0.19874016,  0.20314961,  0.19291339,  0.2015748 ],\n",
       "         [ 0.2050592 ,  0.20057409,  0.20416218,  0.19931826,  0.19088626],\n",
       "         [ 0.16161636,  0.12121252,  0.25252499,  0.29292882,  0.17171731],\n",
       "         [ 0.21275605,  0.19599628,  0.19553073,  0.19180633,  0.20391061],\n",
       "         [ 0.19925214,  0.19497863,  0.19604701,  0.20940171,  0.20032051],\n",
       "         [ 0.17703355,  0.20574161,  0.21052629,  0.16746419,  0.23923436],\n",
       "         [ 0.11111182,  0.22222205,  0.20634916,  0.22222205,  0.23809494],\n",
       "         [ 0.20083185,  0.20063379,  0.19489008,  0.21311151,  0.19053278],\n",
       "         [ 0.19051463,  0.21029263,  0.19475278,  0.20464178,  0.19979818],\n",
       "         [ 0.20718069,  0.18756131,  0.2005101 ,  0.20521876,  0.19952913],\n",
       "         [ 0.1904762 ,  0.1883117 ,  0.2380952 ,  0.1796537 ,  0.2034632 ],\n",
       "         [ 0.19226019,  0.20008234,  0.19061342,  0.20337587,  0.21366817],\n",
       "         [ 0.20104287,  0.19872538,  0.20239475,  0.20413287,  0.19370413],\n",
       "         [ 0.20705016,  0.19368584,  0.20918071,  0.19446059,  0.1956227 ],\n",
       "         [ 0.19812516,  0.20521915,  0.19901191,  0.19977198,  0.1978718 ],\n",
       "         [ 0.19855305,  0.19352894,  0.19915595,  0.20458199,  0.20418006],\n",
       "         [ 0.20174849,  0.19816185,  0.19390271,  0.20242098,  0.20376597],\n",
       "         [ 0.20975609,  0.18699188,  0.20813007,  0.19837399,  0.19674797],\n",
       "         [ 0.19220945,  0.20019157,  0.20338442,  0.20977011,  0.19444445],\n",
       "         [ 0.20646319,  0.20466786,  0.19389588,  0.19389588,  0.2010772 ],\n",
       "         [ 0.21071752,  0.20526794,  0.1907357 ,  0.18982743,  0.20345141],\n",
       "         [ 0.20846904,  0.18566778,  0.21172637,  0.19218242,  0.20195439],\n",
       "         [ 0.1900474 ,  0.20189573,  0.21753554,  0.207109  ,  0.18341233],\n",
       "         [ 0.19452748,  0.20188549,  0.20326512,  0.19429754,  0.20602437],\n",
       "         [ 0.19509203,  0.20245399,  0.21288343,  0.19754601,  0.19202454],\n",
       "         [ 0.23781675,  0.19103314,  0.19785575,  0.18908383,  0.18421053],\n",
       "         [ 0.18604653,  0.20930231,  0.22739015,  0.20930231,  0.1679587 ],\n",
       "         [ 0.21679908,  0.19977299,  0.18842225,  0.19750284,  0.19750284]])),\n",
       " array([-711200.24005818, -706956.09225089, -707122.76799223, ...,\n",
       "        -708275.96084106, -707339.60406971, -710000.0306799 ]))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_lda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "phi_theta = test_lda2[0]\n",
    "phi = pd.DataFrame(data=phi_theta[0], columns = train_frame.columns)\n",
    "theta = pd.DataFrame(data=phi_theta[1])\n",
    "dat_phi = phi.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>words</th>\n",
       "      <th>abb</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>above</th>\n",
       "      <th>absence</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abstract</th>\n",
       "      <th>academic</th>\n",
       "      <th>academy</th>\n",
       "      <th>...</th>\n",
       "      <th>yobs</th>\n",
       "      <th>yohai</th>\n",
       "      <th>york</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>youth</th>\n",
       "      <th>zeger</th>\n",
       "      <th>zero</th>\n",
       "      <th>zin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.133334</td>\n",
       "      <td>0.282608</td>\n",
       "      <td>0.191781</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.142858</td>\n",
       "      <td>0.312496</td>\n",
       "      <td>0.142858</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.208955</td>\n",
       "      <td>0.363629</td>\n",
       "      <td>0.197403</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.050004</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.272724</td>\n",
       "      <td>0.125002</td>\n",
       "      <td>0.211055</td>\n",
       "      <td>0.090914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.263156</td>\n",
       "      <td>0.333332</td>\n",
       "      <td>0.152174</td>\n",
       "      <td>0.232877</td>\n",
       "      <td>0.189543</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.312496</td>\n",
       "      <td>0.228571</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223880</td>\n",
       "      <td>0.090914</td>\n",
       "      <td>0.184416</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.100002</td>\n",
       "      <td>0.249999</td>\n",
       "      <td>0.181819</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.241206</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.333332</td>\n",
       "      <td>0.239130</td>\n",
       "      <td>0.219178</td>\n",
       "      <td>0.241830</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125002</td>\n",
       "      <td>0.257142</td>\n",
       "      <td>0.266666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.119404</td>\n",
       "      <td>0.090914</td>\n",
       "      <td>0.179221</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.249999</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.272724</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.180905</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210526</td>\n",
       "      <td>0.088890</td>\n",
       "      <td>0.130436</td>\n",
       "      <td>0.197260</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>0.260868</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.062504</td>\n",
       "      <td>0.257142</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223880</td>\n",
       "      <td>0.363629</td>\n",
       "      <td>0.215584</td>\n",
       "      <td>0.178572</td>\n",
       "      <td>0.399995</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.090914</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.165829</td>\n",
       "      <td>0.636344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.105266</td>\n",
       "      <td>0.111112</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.158904</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.086959</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.114287</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223880</td>\n",
       "      <td>0.090914</td>\n",
       "      <td>0.223377</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>0.181819</td>\n",
       "      <td>0.333331</td>\n",
       "      <td>0.201005</td>\n",
       "      <td>0.272724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3569 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "words       abb   ability      able     about     above   absence  absolute  \\\n",
       "0      0.210526  0.133334  0.282608  0.191781  0.215686  0.217391  0.142858   \n",
       "1      0.263156  0.333332  0.152174  0.232877  0.189543  0.217391  0.171429   \n",
       "2      0.210526  0.333332  0.239130  0.219178  0.241830  0.217391  0.200000   \n",
       "3      0.210526  0.088890  0.130436  0.197260  0.215686  0.260868  0.200000   \n",
       "4      0.105266  0.111112  0.195652  0.158904  0.137255  0.086959  0.285714   \n",
       "\n",
       "words  abstract  academic   academy    ...         yobs     yohai      york  \\\n",
       "0      0.312496  0.142858  0.166667    ...     0.208955  0.363629  0.197403   \n",
       "1      0.312496  0.228571  0.200000    ...     0.223880  0.090914  0.184416   \n",
       "2      0.125002  0.257142  0.266666    ...     0.119404  0.090914  0.179221   \n",
       "3      0.062504  0.257142  0.166667    ...     0.223880  0.363629  0.215584   \n",
       "4      0.187500  0.114287  0.200000    ...     0.223880  0.090914  0.223377   \n",
       "\n",
       "words       you     young      your     youth     zeger      zero       zin  \n",
       "0      0.196429  0.050004  0.222222  0.272724  0.125002  0.211055  0.090914  \n",
       "1      0.196429  0.100002  0.249999  0.181819  0.166667  0.241206  0.000009  \n",
       "2      0.214286  0.249999  0.166667  0.272724  0.166667  0.180905  0.000009  \n",
       "3      0.178572  0.399995  0.222222  0.090914  0.208333  0.165829  0.636344  \n",
       "4      0.214286  0.200000  0.138890  0.181819  0.333331  0.201005  0.272724  \n",
       "\n",
       "[5 rows x 3569 columns]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "words\n",
       "snedecor      0.538449\n",
       "exposition    0.529402\n",
       "gets          0.526307\n",
       "averaging     0.499993\n",
       "mcleod        0.499989\n",
       "bls           0.499989\n",
       "marvin        0.499988\n",
       "payments      0.499988\n",
       "residents     0.499988\n",
       "anything      0.499988\n",
       "gupta         0.473677\n",
       "highest       0.470580\n",
       "norman        0.470580\n",
       "wilson        0.466658\n",
       "hastings      0.466658\n",
       "revealed      0.461528\n",
       "mgf           0.461528\n",
       "jersey        0.461528\n",
       "thesis        0.461528\n",
       "Name: 2, dtype: float64"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_ = 2\n",
    "dat_phi = dat_phi.sort_values(col_,ascending=False)\n",
    "dat_phi.iloc[1:20,col_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi_new = pd.DataFrame(sampler2.NZW/np.sum( (sampler2.NZW + sampler2.beta),axis = 1)[:,np.newaxis],columns = train_frame.columns)\n",
    "phi_new.to_csv(\"./Models/topics5/phi_new.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phi.to_csv(\"./Models/topics5/phi.csv\" )\n",
    "theta.to_csv(\"./Models/topics5/theta.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(likelihood).to_csv(\"./Models/topics5/likelihood.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_frame.to_csv(\"./Models/topics5/dtm_dat.csv\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
