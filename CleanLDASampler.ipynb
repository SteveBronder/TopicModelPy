{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import gammaln\n",
    "import scipy.misc\n",
    "import random\n",
    "\n",
    "def index_sample(p):\n",
    "    \"\"\"\n",
    "    Desc: Samples from n topics distributed multinomially and returns topic number\n",
    "    input: p - A one dimensional array of float64 type that contains the probability for each topic\n",
    "    output: an Integer specifying which topic was chosen from a multinomial distribution\n",
    "    \"\"\"\n",
    "    r = random.random()\n",
    "    for i in range(len(p)):\n",
    "        r = r - p[i]\n",
    "        if r < 0:\n",
    "            return i\n",
    "    return len(p) - 1\n",
    "\n",
    "def word_indices(vec):\n",
    "    \"\"\"\n",
    "    Desc: Take a vector of word counts from a document and create a generator for word indices\n",
    "    input: A vector from a Document Term Frequency matrix for one document.\n",
    "    output: A generator object to store the word indices when called\n",
    "    \"\"\"\n",
    "    for idx in vec.nonzero()[0]:\n",
    "        for i in xrange(int(vec[idx])):\n",
    "            yield idx\n",
    "\n",
    "def log_multi_beta(alpha, K = None):\n",
    "    \"\"\"\n",
    "    Desc: Compute the logarithm of the multinomial beta function\n",
    "    input: alpha - A vector with type float64 or a scaler of float64\n",
    "           K - An integer that, if alpha is a scalar, multiplies the log by K\n",
    "    output: a float64 with value of the logarithm of the multinomial beta\n",
    "    \"\"\"\n",
    "\n",
    "    if K is None:\n",
    "        return np.sum(gammaln(alpha) - gammaln(np.sum(alpha)))\n",
    "    else:\n",
    "        return K * gammaln(alpha) - gammaln(K * alpha)\n",
    "\n",
    "class LdaSampler(object):\n",
    "\n",
    "    def __init__(self,  data, ntopics, alpha = .1, beta = .1):\n",
    "        \"\"\"\n",
    "        Desc: Initialize values for our class object\n",
    "        alpha: a float scalar\n",
    "        beta: a float scalar\n",
    "        ntopics: an integer for the number of topics\n",
    "        \"\"\"\n",
    "        if not isinstance(alpha, float):\n",
    "            raise Exception(\" Initial value for alpha must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(beta, float):\n",
    "            raise Exception(\" Initial value for beta must be a floating point number (.3)\")\n",
    "\n",
    "        if not isinstance(ntopics, int):\n",
    "            raise Exception(\" The number of topics must be an integer\")\n",
    "\n",
    "        self.matrix = data\n",
    "        self.ntopics = ntopics\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self._initialize()\n",
    "    def _initialize(self):\n",
    "        \"\"\"\n",
    "        Initialize:\n",
    "        NZM: size(#Docs X #Topics) numpy array with type float 64\n",
    "            The number of times document M and topic Z interact\n",
    "\n",
    "        NZW: size(#Topics X #Words) numpy array with type float64\n",
    "            The number of times topic Z and word W interact\n",
    "\n",
    "        NM:  size(#Docs) numpy array with type float64\n",
    "            Sum of documents occurances by topic and word\n",
    "\n",
    "        NZ:  size(#Topics) numpy array with type float64\n",
    "            Sum of Topic occurences by word and document\n",
    "\n",
    "        Topics: size(?) An empty set\n",
    "           Will come back to this\n",
    "        \"\"\"\n",
    "        ndocs, vsize = self.matrix.shape\n",
    "\n",
    "        self.NZM = np.zeros((ndocs, self.ntopics))\n",
    "        self.NZW = np.zeros((self.ntopics, vsize))\n",
    "        self.NM  = np.zeros(ndocs)\n",
    "        self.NZ  = np.zeros(self.ntopics)\n",
    "        self.topics = {}\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            # Iterates over i, doc_length - 1, and w, the size of unique_words - 1\n",
    "            for i, w, in enumerate(word_indices(self.matrix[m,:])):\n",
    "                # Initialize a random topic for each word\n",
    "                z = np.random.randint(self.ntopics)\n",
    "                self.NZM[m,z] += 1\n",
    "                # Why is NM being +1'd for each i,w?\n",
    "                self.NM[m] += 1\n",
    "                self.NZW[z,w] += 1\n",
    "                self.NZ[z] += 1\n",
    "                self.topics[(m,i)] = z\n",
    "\n",
    "    def _conditional_distribution(self, m, w):\n",
    "        \"\"\"\n",
    "        Desc: Compute the conditional distribution of words in document and topic\n",
    "        Input: m: An integer representing the column index of the document\n",
    "               w: The generator object from word_indices\n",
    "\n",
    "        Output: p_z: An array size(w X 1) containing probabilities for topics of word\n",
    "        \"\"\"\n",
    "        vsize = self.NZW.shape[1]\n",
    "        left = (self.NZW[:,w] + self.beta) / (self.NZ + self.beta * vsize)\n",
    "        right = (self.NZM[m,:] + self.alpha) / (self.NM[m] + self.alpha * self.ntopics)\n",
    "        p_z = abs(left * right)\n",
    "        p_z /= np.sum(p_z)\n",
    "        return p_z\n",
    "\n",
    "    def loglikelihood(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute the log likelihood that the model generated the data\n",
    "        Input: self references\n",
    "        Output: lik: float of the log likelihood\n",
    "        \"\"\"\n",
    "        # Why are these being repeated here?\n",
    "        vsize = self.NZW.shape[1]\n",
    "        ndocs = self.NZM.shape[0]\n",
    "        lik = 0\n",
    "\n",
    "        for z in xrange(self.ntopics):\n",
    "            lik += log_multi_beta(self.NZW[z,:] + self.beta)\n",
    "            lik -= log_multi_beta(self.beta, vsize)\n",
    "\n",
    "        for m in xrange(ndocs):\n",
    "            lik += log_multi_beta(self.NZM[m,:] + self.alpha)\n",
    "            lik -= log_multi_beta(self.alpha, self.ntopics)\n",
    "\n",
    "        return lik\n",
    "\n",
    "    def phi_theta(self):\n",
    "        \"\"\"\n",
    "        Desc: Compute phi and theta, our topic by word probs and document by topic probs\n",
    "        Input: Self references\n",
    "        Output: Two arrays, holding\n",
    "            [0] phi: Probability of topic by word\n",
    "            [1] theta: Probability of document by topic\n",
    "        \"\"\"\n",
    "        num_phi = self.NZW + self.beta\n",
    "        num_phi /= np.sum(num_phi, axis = 1)[:, np.newaxis]\n",
    "\n",
    "        num_theta = self.NZM + self.alpha\n",
    "        num_theta /= np.sum(num_theta,axis = 1)[:, np.newaxis]\n",
    "\n",
    "        return num_phi, num_theta\n",
    "\n",
    "\n",
    "    def run(self, maxiter = 30, burnin= 0):\n",
    "        \"\"\"\n",
    "        Desc: Perform Gibbs sampling for maxiter iterations\n",
    "\n",
    "        Input: matrix - An array that is a Document Term Frequency Matrix\n",
    "               maxiter - An integer with the number of iterations\n",
    "               Burnin - TBA: An integer of the number of burnins\n",
    "\n",
    "        Output: phi_theta() Two arrays, holding\n",
    "        [0] Probability of topic by word\n",
    "        [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "\n",
    "        n_docs, vsize = self.matrix.shape\n",
    "\n",
    "\n",
    "\n",
    "        for iteration in xrange(maxiter):\n",
    "            for m in xrange(n_docs):\n",
    "                for i,w in enumerate(word_indices(self.matrix[m,:])):\n",
    "                    z = self.topics[(m,i)]\n",
    "\n",
    "                    self.NZM[m,z] -= 1\n",
    "                    self.NM[m] -= 1\n",
    "                    self.NZW[z,w] -= 1\n",
    "                    self.NZ[z] -= 1\n",
    "\n",
    "                    p_z = self._conditional_distribution(m,w)\n",
    "                    z = index_sample(p_z)\n",
    "\n",
    "                    self.NZM[m,z] += 1\n",
    "                    self.NM[m] += 1\n",
    "                    self.NZW[z,w] += 1\n",
    "                    self.NZ[z] += 1\n",
    "\n",
    "            if iteration > burnin:\n",
    "                yield self.phi_theta()\n",
    "\n",
    "    def prn(self,x = None):\n",
    "        print x\n",
    "\n",
    "    # For some reason this returns (maxiter - burnin) - 2 iterations?\n",
    "    def update(self, maxiter = 20, burnin = 0):\n",
    "        \"\"\"\n",
    "        Desc: Runs gibbs sampler for maxiter iterations\n",
    "            Input: maxiter - integer specifying maximum number of iterations\n",
    "                   burnin  - integer specifying number of iterations to burn through.\n",
    "                                should be set to zero after initial burnin\n",
    "            Output: phi_theta() Two arrays, holding\n",
    "                [0] Probability of topic by word\n",
    "                [1] Probability of document by topic\n",
    "        \"\"\"\n",
    "        for iteration, phi_theta in enumerate(self.run( maxiter, burnin)):\n",
    "            self.prn(iteration)\n",
    "            self.prn(self.loglikelihood())\n",
    "        return self.phi_theta()\n",
    "\n",
    "    def __call__(self):\n",
    "        self.NZM = self.NZM\n",
    "        self.NM = self.NM\n",
    "        self.NZW = self.NZW\n",
    "        self.NZ = self.NZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.special import gammaln\n",
    "\n",
    "DIR = r'data_folder/wordcounts'\n",
    "allfiles = glob.glob(os.path.join(DIR,\"*.CSV\"))\n",
    "\n",
    "p = .5\n",
    "rand_sample = [ allfiles[i] for i in sorted(random.sample(xrange(len(allfiles)), int(p * len(allfiles)))) ]\n",
    "rand_sample\n",
    "\n",
    "np_array_list = []\n",
    "for file_ in rand_sample:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    df['source'] = file_\n",
    "    np_array_list.append(df.as_matrix())\n",
    "\n",
    "comb_np_array = np.vstack(np_array_list)\n",
    "big_frame = pd.DataFrame(comb_np_array)\n",
    "big_frame.columns = ['words','count','source']\n",
    "big_frame = big_frame.fillna(value = 0)\n",
    "big_frame = big_frame.pivot(index = 'source',columns = 'words', values = 'count')\n",
    "big_frame = big_frame.fillna(value = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words                                                  0    a  aa  aaa  \\\n",
      "source                                                                   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV  0    7   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV  0  181   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV  0    4   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV  0   28   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV  0   78   0    0   \n",
      "\n",
      "words                                                  aaaaaaemh  aaaai  aaas  \\\n",
      "source                                                                          \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV          0      0     0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV          0      0     0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV          0      0     0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV          0      0     0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV          0      0     0   \n",
      "\n",
      "words                                                  aaby  aachen  aaees  \\\n",
      "source                                                                       \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV     0       0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV     0       0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV     0       0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV     0       0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV     0       0      0   \n",
      "\n",
      "words                                                  ...   zygotic  zyl  \\\n",
      "source                                                 ...                  \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV  ...         0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV  ...         0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV  ...         0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV  ...         0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV  ...         0    0   \n",
      "\n",
      "words                                                  zylkzkt  zymax  zymin  \\\n",
      "source                                                                         \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV        0      0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV        0      0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV        0      0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV        0      0      0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV        0      0      0   \n",
      "\n",
      "words                                                  zyskind  zyv  zz  zzi  \\\n",
      "source                                                                         \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV        0    0   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV        0    0   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV        0    0   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV        0    0   0    0   \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV        0    0   0    0   \n",
      "\n",
      "words                                                  zzzo  \n",
      "source                                                       \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276732.CSV     0  \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276856.CSV     0  \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276867.CSV     0  \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276929.CSV     0  \n",
      "data_folder/wordcounts/wordcounts_10.2307_2276953.CSV     0  \n",
      "\n",
      "[5 rows x 35280 columns]\n"
     ]
    }
   ],
   "source": [
    "print big_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "-545292582237.0\n",
      "1\n",
      "-545293011821.0\n",
      "2\n",
      "-545312574588.0\n",
      "3\n",
      "-545400695781.0\n",
      "4\n",
      "-545489208551.0\n",
      "5\n",
      "-545535945630.0\n",
      "6\n",
      "-545552886080.0\n",
      "7\n",
      "-545554314346.0\n",
      "8\n",
      "-545550136627.0\n",
      "9\n",
      "-545532161016.0\n",
      "10\n",
      "-545501929152.0\n",
      "11\n",
      "-545471553852.0\n",
      "12\n",
      "-545452008458.0\n",
      "13\n",
      "-545436938583.0\n",
      "14\n",
      "-545426989614.0\n",
      "15\n",
      "-545427185471.0\n",
      "16\n",
      "-545428018413.0\n",
      "17\n",
      "-545432969125.0\n",
      "18\n",
      "-545446207762.0\n",
      "time elapsed:  505.472303152  seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "sampler = LdaSampler(data = big_frame.values, ntopics = 5, alpha = .1, beta = .1)\n",
    "LDAtest = sampler.update(maxiter = 20)\n",
    "\n",
    "elapsed = timeit.default_timer() - start_time\n",
    "print \"time elapsed: \", elapsed, \" seconds\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[ -3.86560200e-05,  -1.15987193e-01,  -1.98746608e-04, ...,\n",
      "         -1.47986178e-04,  -8.94164503e-05,   1.60090588e-05],\n",
      "       [  1.74904761e-04,  -1.09700197e-01,   7.60202492e-04, ...,\n",
      "         -3.73084519e-05,   3.42279375e-07,   3.42279375e-07],\n",
      "       [  4.07698988e-07,  -1.13894382e-01,  -1.13748018e-04, ...,\n",
      "          4.40722606e-04,   4.07698988e-07,   4.07698988e-07],\n",
      "       [ -3.61769284e-05,  -1.06260268e-01,  -2.78462779e-04, ...,\n",
      "         -1.12513566e-04,   1.33091269e-04,   3.31898426e-07],\n",
      "       [ -1.00036560e-04,   6.11975059e-01,  -8.39664295e-05, ...,\n",
      "         -4.78086369e-05,  -5.18261694e-05,  -1.16508443e-05]]), array([[ 5.96203606, -1.23414634, -1.18536585, -1.43563097, -1.1068929 ],\n",
      "       [-0.9226317 , -1.01635503, -0.99463108,  4.36854682, -0.43492901],\n",
      "       [-1.14200937,  1.96262363, -0.99521083, -1.05143155,  2.22602811],\n",
      "       ..., \n",
      "       [ 6.20363224, -1.32342036, -1.29920545, -1.31812334, -1.26288309],\n",
      "       [ 1.90867925, -1.01039832, -0.91312369, -1.10096436,  2.11580713],\n",
      "       [-0.93608262,  2.4717923 , -0.99159435, -1.02716775,  1.48305243]]))\n"
     ]
    }
   ],
   "source": [
    "print LDAtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
